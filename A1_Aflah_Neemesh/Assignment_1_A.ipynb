{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xURHYz8XsLoR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "k-ptEnijslOo",
        "outputId": "7077eda8-1c04-4a89-f17e-ea5b286477a5"
      },
      "outputs": [],
      "source": [
        "df_data = pd.read_csv('A1_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RBquCbrZsny5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LABEL</th>\n",
              "      <th>DATE_TIME</th>\n",
              "      <th>TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Fri Jun 05 14:26:50 2009</td>\n",
              "      <td>About to get threaded and scared</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Thu May 14 10:13:55 2009</td>\n",
              "      <td>@awaisnaseer I like Shezan Mangooo too!!! I ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Fri Jun 05 21:02:20 2009</td>\n",
              "      <td>worked on my car after work. showering then go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Sun Jun 14 22:25:52 2009</td>\n",
              "      <td>@Marama Actually we start this afternoon!  I w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Sun May 31 00:42:12 2009</td>\n",
              "      <td>@gfalcone601 Aww Gi.don't worry.we'll vote for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4282</th>\n",
              "      <td>1</td>\n",
              "      <td>Sat Jun 06 22:45:26 2009</td>\n",
              "      <td>@QandQ My performances on my CLEP tests.  #qshock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4283</th>\n",
              "      <td>0</td>\n",
              "      <td>Tue Jun 16 10:17:07 2009</td>\n",
              "      <td>ugh no, rcn had all the true blood episodes on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4284</th>\n",
              "      <td>1</td>\n",
              "      <td>Fri May 01 22:00:42 2009</td>\n",
              "      <td>Just returned from the forest! Sarah (my merch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4285</th>\n",
              "      <td>1</td>\n",
              "      <td>Sun Jun 07 02:09:46 2009</td>\n",
              "      <td>is proud of her dad and his piece of work. ( h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4286</th>\n",
              "      <td>0</td>\n",
              "      <td>Fri May 22 04:49:37 2009</td>\n",
              "      <td>Just woke up, gonna eat pizza for breakfast. A...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4287 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      LABEL                 DATE_TIME  \\\n",
              "0         0  Fri Jun 05 14:26:50 2009   \n",
              "1         1  Thu May 14 10:13:55 2009   \n",
              "2         1  Fri Jun 05 21:02:20 2009   \n",
              "3         1  Sun Jun 14 22:25:52 2009   \n",
              "4         1  Sun May 31 00:42:12 2009   \n",
              "...     ...                       ...   \n",
              "4282      1  Sat Jun 06 22:45:26 2009   \n",
              "4283      0  Tue Jun 16 10:17:07 2009   \n",
              "4284      1  Fri May 01 22:00:42 2009   \n",
              "4285      1  Sun Jun 07 02:09:46 2009   \n",
              "4286      0  Fri May 22 04:49:37 2009   \n",
              "\n",
              "                                                   TEXT  \n",
              "0                     About to get threaded and scared   \n",
              "1     @awaisnaseer I like Shezan Mangooo too!!! I ha...  \n",
              "2     worked on my car after work. showering then go...  \n",
              "3     @Marama Actually we start this afternoon!  I w...  \n",
              "4     @gfalcone601 Aww Gi.don't worry.we'll vote for...  \n",
              "...                                                 ...  \n",
              "4282  @QandQ My performances on my CLEP tests.  #qshock  \n",
              "4283  ugh no, rcn had all the true blood episodes on...  \n",
              "4284  Just returned from the forest! Sarah (my merch...  \n",
              "4285  is proud of her dad and his piece of work. ( h...  \n",
              "4286  Just woke up, gonna eat pizza for breakfast. A...  \n",
              "\n",
              "[4287 rows x 3 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "nHxGLvCZbejn",
        "outputId": "7d6f0aa3-570e-4cdb-ec26-b877e93edd4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['About to get threaded and scared ',\n",
              " '@awaisnaseer I like Shezan Mangooo too!!! I had one yesterday ',\n",
              " 'worked on my car after work. showering then going to bed. sooooooooooo tired. sparrow signing out  &lt;Cowboy Up&gt;',\n",
              " '@Marama Actually we start this afternoon!  I will try to have something by Wed. It will be a slow process of collecting, many on vac.',\n",
              " \"@gfalcone601 Aww Gi.don't worry.we'll vote for you non-stop coz we love you so much \",\n",
              " '@mrstessyman What ever you do have a good day. I love knitpicks ',\n",
              " '@GetMeVideo Sorry, not my forte,  Ask me about #trading, #scalping the dax and I can answer ',\n",
              " \"Getting ready for church and bummed I cannot watch Rafa whoop Djokovic LIVE, I'll settle for DVR I suppose \",\n",
              " 'Up early tomorrow. Last open home. Goodnight ',\n",
              " \"Needs to shake this gloomy feeling!!    Maybe it's the rain???\"]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data['TEXT'].to_list()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "woQssMfKAQe3"
      },
      "outputs": [],
      "source": [
        "from customregexes import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAVPGC_8rRM7"
      },
      "source": [
        "# I. REGULAR EXPRESSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n87jQodrSJ5"
      },
      "source": [
        "# A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtiD3IRnsvR0"
      },
      "source": [
        "## a. Average number of sentences and tokens \n",
        "- ! or ? (Even Continous)\n",
        "- . Followed by one or more spaces and then a capital character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1-38IM7hfmVc"
      },
      "outputs": [],
      "source": [
        "df_data['sentence_count'] = df_data['TEXT'].apply(lambda x: findSentenceCount(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LABEL\n",
              "0    1.803000\n",
              "1    1.856143\n",
              "Name: sentence_count, dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data.groupby(['LABEL'])['sentence_count'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def findTokens(sentence):\n",
        "  # Split on Whitespace using regex\n",
        "  tokens = re.split(\"\\s+\", sentence)\n",
        "  # Check URLs and Users\n",
        "  allTokens = []\n",
        "  for token in tokens:\n",
        "    if findURLs(token):\n",
        "      URLs = findURLs(token)\n",
        "      for url in URLs:\n",
        "        indexes = re.finditer(url, token)\n",
        "        for index in indexes:\n",
        "          allTokens.append(token[index.start():index.end()])\n",
        "          allTokens.append(token[:index.start()])\n",
        "          allTokens.append(token[index.end():])\n",
        "    elif findUsernames(token):\n",
        "      usernames = findUsernames(token)\n",
        "      for username in usernames:\n",
        "        indexes = re.finditer(username, token)\n",
        "        for index in indexes:\n",
        "          allTokens.append(token[index.start():index.end()])\n",
        "          allTokens.append(token[:index.start()])\n",
        "          allTokens.append(token[index.end():])\n",
        "    else:\n",
        "      # split on punctuation\n",
        "      if re.search(\"[\\.!?]+\", token):\n",
        "        allTokens.extend(re.split(\"([\\.!?]+)\", token))\n",
        "      else:\n",
        "        allTokens.append(token)\n",
        "  # Remove empty strings\n",
        "  new_token_components = []\n",
        "  for i in range(len(allTokens)):\n",
        "    if not match_empty(allTokens[i]):\n",
        "      new_token_components.append(allTokens[i])\n",
        "  # Remove leading and trailing spaces\n",
        "  allTokens = new_token_components\n",
        "  for i in range(len(allTokens)):\n",
        "    allTokens[i] = lrstrip(allTokens[i])\n",
        "  return allTokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['@awaisnaseer',\n",
              " 'I',\n",
              " 'like',\n",
              " 'Shezan',\n",
              " 'Mangooo',\n",
              " 'too',\n",
              " '!!!',\n",
              " 'I',\n",
              " 'had',\n",
              " 'one',\n",
              " 'yesterday',\n",
              " 'www.google.com',\n",
              " '.',\n",
              " 'Helllooo',\n",
              " 'Gi',\n",
              " '.',\n",
              " \"don't\"]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = \"@awaisnaseer I like Shezan Mangooo too!!! I had one yesterday www.google.com. Helllooo Gi.don't\"\n",
        "findTokens(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GiDosJOaRAgw"
      },
      "outputs": [],
      "source": [
        "df_data['token_count'] = df_data['TEXT'].apply(lambda x: findTokenCount(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_data['tokens'] = df_data['TEXT'].apply(lambda x: findTokens(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXZd-DZcsu_4",
        "outputId": "0d516035-5bdb-4aab-fc08-f6d6d3ba0f8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LABEL\n",
              "0    13.605500\n",
              "1    12.829908\n",
              "Name: token_count, dtype: float64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data.groupby(['LABEL'])['token_count'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>About to get threaded and scared</td>\n",
              "      <td>[About, to, get, threaded, and, scared]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@awaisnaseer I like Shezan Mangooo too!!! I ha...</td>\n",
              "      <td>[@awaisnaseer, I, like, Shezan, Mangooo, too, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>worked on my car after work. showering then go...</td>\n",
              "      <td>[worked, on, my, car, after, work, ., showerin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Marama Actually we start this afternoon!  I w...</td>\n",
              "      <td>[@Marama, Actually, we, start, this, afternoon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@gfalcone601 Aww Gi.don't worry.we'll vote for...</td>\n",
              "      <td>[@gfalcone601, Aww, Gi, ., don't, worry, ., we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4282</th>\n",
              "      <td>@QandQ My performances on my CLEP tests.  #qshock</td>\n",
              "      <td>[@QandQ, My, performances, on, my, CLEP, tests...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4283</th>\n",
              "      <td>ugh no, rcn had all the true blood episodes on...</td>\n",
              "      <td>[ugh, no,, rcn, had, all, the, true, blood, ep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4284</th>\n",
              "      <td>Just returned from the forest! Sarah (my merch...</td>\n",
              "      <td>[Just, returned, from, the, forest, !, Sarah, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4285</th>\n",
              "      <td>is proud of her dad and his piece of work. ( h...</td>\n",
              "      <td>[is, proud, of, her, dad, and, his, piece, of,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4286</th>\n",
              "      <td>Just woke up, gonna eat pizza for breakfast. A...</td>\n",
              "      <td>[Just, woke, up,, gonna, eat, pizza, for, brea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4287 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   TEXT  \\\n",
              "0                     About to get threaded and scared    \n",
              "1     @awaisnaseer I like Shezan Mangooo too!!! I ha...   \n",
              "2     worked on my car after work. showering then go...   \n",
              "3     @Marama Actually we start this afternoon!  I w...   \n",
              "4     @gfalcone601 Aww Gi.don't worry.we'll vote for...   \n",
              "...                                                 ...   \n",
              "4282  @QandQ My performances on my CLEP tests.  #qshock   \n",
              "4283  ugh no, rcn had all the true blood episodes on...   \n",
              "4284  Just returned from the forest! Sarah (my merch...   \n",
              "4285  is proud of her dad and his piece of work. ( h...   \n",
              "4286  Just woke up, gonna eat pizza for breakfast. A...   \n",
              "\n",
              "                                                 tokens  \n",
              "0               [About, to, get, threaded, and, scared]  \n",
              "1     [@awaisnaseer, I, like, Shezan, Mangooo, too, ...  \n",
              "2     [worked, on, my, car, after, work, ., showerin...  \n",
              "3     [@Marama, Actually, we, start, this, afternoon...  \n",
              "4     [@gfalcone601, Aww, Gi, ., don't, worry, ., we...  \n",
              "...                                                 ...  \n",
              "4282  [@QandQ, My, performances, on, my, CLEP, tests...  \n",
              "4283  [ugh, no,, rcn, had, all, the, true, blood, ep...  \n",
              "4284  [Just, returned, from, the, forest, !, Sarah, ...  \n",
              "4285  [is, proud, of, her, dad, and, his, piece, of,...  \n",
              "4286  [Just, woke, up,, gonna, eat, pizza, for, brea...  \n",
              "\n",
              "[4287 rows x 2 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data[['TEXT', 'tokens']].to_csv('A1_dataset_with_tokens.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DCL4GeyBb2B"
      },
      "source": [
        "## b. Words Starting With Consonants or Vowels\n",
        "\n",
        "Basic Rules and Assumptions:\n",
        "\n",
        "- Word Boundary or Punctuation then a Vowel followed by only Alphabets (accented-non-accented both) and nothing else so A9A is not a word\n",
        "\n",
        "- Word Boundary or Punctuation then a Consonant followed by only Alphabets (accented-non-accented both) and nothing else so A9A is not a word\n",
        "\n",
        "- Compound Words like truck-driver can also be there\n",
        "\n",
        "- Since it is Twitter data the data can be multilingual/can have non ascii characters (stuff like accented characters) and they can be handled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "A0SSW2qXJYdd"
      },
      "outputs": [],
      "source": [
        "def findWordsStartingWithVowel(sentence):\n",
        "  return len(re.findall(\"\\\\b[aeiouAEIOU][\\w|Ã€-Ã–|Ã˜-Ã¶|Ã¸-Ã¿|'|-]*\\\\b\", sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zLOdmzhCGVW",
        "outputId": "154d9bf6-100f-40aa-e3f8-0f7389880743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Aww', \"Aon't\", \"ae'll\", 'UUUU', 'and', 'IM', 'A9A', 'ae-l', 'ee', 'ef', 'aruck-ariver']\n"
          ]
        }
      ],
      "source": [
        "findWordsStartingWithVowel(\"Aww Gi...Aon't worry:ae'll \\n vote for UUUU and IM. A9A ae-l ee:ef aruck-ariver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "21bourgDKKyL"
      },
      "outputs": [],
      "source": [
        "df_data['words_starting_with_vowel'] = df_data['TEXT'].apply(lambda x: findWordsStartingWithVowel(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIUx_RhlqJkQ",
        "outputId": "1220f13f-7b0f-4796-eda8-53e5c875cfd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14177"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data['words_starting_with_vowel'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LABEL\n",
              "0    6989\n",
              "1    7188\n",
              "Name: words_starting_with_vowel, dtype: int64"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data.groupby(['LABEL'])['words_starting_with_vowel'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X38Xu10RN-Pu"
      },
      "outputs": [],
      "source": [
        "def findWordsStartingWithConsonant(sentence):\n",
        "  return len(re.findall(\"\\\\s([bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ][\\w|Ã€-Ã–|Ã˜-Ã¶|Ã¸-Ã¿|'|-]*)\\\\b\", sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enbkdMy9Pojv",
        "outputId": "70d825dd-5c47-4ae0-a41d-a6eb842a167c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findWordsStartingWithConsonant(\"Aww Gi...Aon't worry:ae'll \\n vote for UUUU and IM. A9A ae-l ee:ef aruck-ariver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wibwwaouCJDC"
      },
      "outputs": [],
      "source": [
        "df_data['words_starting_with_consonant'] = df_data['TEXT'].apply(lambda x: findWordsStartingWithConsonant(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ookPUBYcqHY8",
        "outputId": "f60386ca-600a-4767-8c56-79dc666d52b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37196"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data['words_starting_with_consonant'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYRKw0v3Pq2n"
      },
      "source": [
        "## c. Lowercase the text and report the number of unique tokens present before and after lower casing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unique Tokens Before Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Fa4UbCdpPOoL"
      },
      "outputs": [],
      "source": [
        "def get_unique_tokens(tokens):\n",
        "    unique_tokens = set()\n",
        "    for token_list in tokens:\n",
        "        for token in token_list:\n",
        "            unique_tokens.add(token)\n",
        "    return len(unique_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = df_data['tokens'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12612"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_unique_tokens(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lowercase(text):\n",
        "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    uppercase = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "    for i in range(26):\n",
        "        text = re.sub(uppercase[i], lowercase[i], text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_data['lowercase_text'] = df_data['TEXT'].apply(lambda x: lowercase(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_data['lowercase_tokens'] = df_data['lowercase_text'].apply(lambda x: findTokens(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "lowercase_tokens = df_data['lowercase_tokens'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11015"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_unique_tokens(lowercase_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdc4Ufy2XiVE"
      },
      "source": [
        "## d. Count and list all the usernames.\n",
        "\n",
        "Basic Rules and Assumptions:\n",
        "\n",
        "- According to [Twitter Guidelines](https://help.twitter.com/en/managing-your-account/twitter-username-rules)\n",
        "  - Your username cannot be longer than 15 characters. Your name can be longer (50 characters) or shorter than 4 characters, but usernames are kept shorter for the sake of ease.\n",
        "  - A username can only contain alphanumeric characters (letters A-Z, numbers 0-9) with the exception of underscores, as noted above. Check to make sure your desired username doesn't contain any symbols, dashes, or spaces.\n",
        "  - *Optional Rule to Spot Users* - Usernames containing the words Twitter or Admin cannot be claimed. No account names can contain Twitter or Admin unless they are official Twitter accounts.\n",
        "\n",
        "- Some experimentation with our Twitter Handle helped us reach the following conclusions:\n",
        "  - @UserName can't be placed with other alphanumeric characters so abc@user 9@user are not valid and won't tag the user\n",
        "  - X@UserName where X is a punctuation is valid but the following cases also don't allow tagging - \n",
        "    - @@xyz\n",
        "    - _@xyz\n",
        "\n",
        "Hence we propose regexes which actually find real tag matches instead of say a user just writing an @ somewhere in the tweet and it getting falsely matched as a tagged user when it really isn't"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6xfqm4QXk3Q"
      },
      "outputs": [],
      "source": [
        "s = ' @awaisnaseer I like Shezan Mangooo too!!! I '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmsSHW76XqZU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxPV1weuX1rn"
      },
      "outputs": [],
      "source": [
        "df_data['UserNames'] = df_data['TEXT'].apply(lambda x: findUsernames(x))\n",
        "df_data['UserNamesCounts'] = df_data['TEXT'].apply(lambda x: findUsernameCount(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIRVTSpZqmDL",
        "outputId": "9d11b68b-019f-4731-c556-66324d14d6c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2108"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data['UserNamesCounts'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IQi6p5SqsHy"
      },
      "outputs": [],
      "source": [
        "usernames = df_data['UserNames'].to_numpy()\n",
        "flattened_usernames = []\n",
        "for i in usernames:\n",
        "  flattened_usernames.extend(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We06hPGvrYVB",
        "outputId": "3ec7df54-41ab-4cde-8bf6-2aa8f5bc7cea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2021"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(flattened_usernames))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9D-7fwqgYNe"
      },
      "source": [
        "## e. Count and list all the urls\n",
        "\n",
        "Basic Rules and Assumptions\n",
        "\n",
        "- `http`, `https`, `www` common starters for URLs usually\n",
        "- Manually investigating all sentences which contain `http` shows no false positives that is all occurences of `http` correspond to links\n",
        "- Manually investigating all sentences which contain `www` shows lots of false positives due to words like `aww`\n",
        "- On manual inspection we find no sentences which contain `https`\n",
        "- According to [Twitter's official Blog](https://help.twitter.com/en/using-twitter/url-shortener), Twitter uses a URL-Shortener which converts links to the form `t.co` however there are no positive matches for this in our dataset. The only matches that arise are spurious matches in links like `blogspot.com`\n",
        "- A URL maybe as simple as `www.xyz.abc` and as complex as `http://www.xyz.abc/efg` and can get even more complex by adding / to index more indepth into pages\n",
        "- There are false positives that we encounter such as `Gi.don` or `worry.we` but they are both valid URLs as well as there can be custom domains by those names. We assume the domain will be atleast 2 characters long to account for `.me`, `.uk` etc.\n",
        "- Numbers have been allowed as [only numbers can form valid URLs](https://stackoverflow.com/q/56804936/13858953)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [],
      "source": [
        "from regexes import findURLs, findURLCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['http://twendz.com']"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findURLs(\"@katgkionis: FollowFriday is a blind 'Shout Out' recomendng ppl U like 2 follow.Goto http://twendz.com &amp; srch 4 followfriday 2 C! \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCOMFaBHy5ou",
        "outputId": "79fe2cbf-641a-41ea-b17c-5fba692803ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['https://www.google.c.uk.in',\n",
              " 'Uk.in',\n",
              " 'www.google.c.uk.in',\n",
              " 'www.google.c.uk',\n",
              " 'http://apps.facebook.com/catbook/profile/view/620328',\n",
              " 'http://www.hakkastudy.in.th',\n",
              " 'http://in.groups.yahoo.com/group/TheComicsClub']"
            ]
          },
          "execution_count": 265,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findURLs(\"      https://www.google.c.uk.in 1.91 1.com com.1 http://www.Google.Co.Uk.in :://www.google.c.uk.in gogole.c www.google.c.uk the best http://apps.facebook.com/catbook/profile/view/620328 ( http://www.hakkastudy.in.th/) http://in.groups.yahoo.com/group/TheComicsClub/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "Pzjw7gUbgj3H"
      },
      "outputs": [],
      "source": [
        "df_data['URLs'] = df_data['TEXT'].apply(lambda x: findURLs(x))\n",
        "df_data['URLCounts'] = df_data['TEXT'].apply(lambda x: findURLCount(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "execution_count": 260,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data['URLCounts'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWU3UgeYzAMn",
        "outputId": "05476f13-c5ff-4e27-daec-4d3279fd39b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['http://bit.ly/AEbs3']\n",
            "['http://twitpic.com/3l589']\n",
            "['http://blip.fm/~4lfcc']\n",
            "['http://bit.ly/n4wL4']\n",
            "['http://bit.ly/rwoHR']\n",
            "['http://su.pr/1rXuPY']\n",
            "['http://twitpic.com/6b03x']\n",
            "['http://tinyurl.com/dk5p94']\n",
            "['http://leo.lobato.org/Blipster']\n",
            "['http://twitpic.com/4ijt4']\n",
            "['http://bit.ly/nZZQV', 'http://bit.ly/etD3a']\n",
            "['http://tinyurl.com/ncbmmo']\n",
            "['http://dontkillspike.proboards.com']\n",
            "['http://twitpic.com/6u8ht']\n",
            "['ball.com/twitter']\n",
            "['http://twendz.com']\n",
            "['http://bit.ly/47etHn']\n",
            "['http://twitpic.com/66zex']\n",
            "['digg.com']\n",
            "['http://twitpic.com/6fs89']\n",
            "['http://bit.ly/i9lsr']\n",
            "['http://www.myspace.com/xautomaticgirlx']\n",
            "['www.musiqtone.com']\n",
            "['http://twitpic.com/5exx2']\n",
            "['http://plurk.com/p/z0xer']\n",
            "['http://mypict.me/2dG2']\n",
            "['http://blip.fm/~4kokb']\n",
            "['http://twitpic.com/54r0g']\n",
            "['http://apps.facebook.com/dogbook/profile/view/6391349']\n",
            "['http://tinyurl.com/pboph6']\n",
            "['http://is.gd/QAaz']\n",
            "['www.paramore.net/shows']\n",
            "['http://twitpic.com/680rp']\n",
            "['http://tinyurl.com/mtq5u2']\n",
            "['http://ustre.am/2FUW']\n",
            "['http://twitpic.com/6h6aw']\n",
            "['http://twitpic.com/6ddox']\n",
            "['www.m2e.asia']\n",
            "['http://ustre.am/2txz']\n",
            "['http://bit.ly/H6RNb']\n",
            "['http://bit.ly/1G5txF']\n",
            "['http://tinyurl.com/nsfan3']\n",
            "['http://yfrog.com/0fvoqj']\n",
            "['SKETCH.ca']\n",
            "['http://plurk.com/p/sy92g']\n",
            "['http://twitpic.com/6uohm']\n",
            "['http://twitpic.com/54r6e']\n",
            "['http://plurk.com/p/xfn8o']\n",
            "['www.inDplay.com']\n",
            "['http://bnup2.com/p/569236']\n",
            "['http://myloc.me/21Sd']\n",
            "['http://twitpic.com/6dty8']\n",
            "['http://bit.ly/Jsxm']\n",
            "['http://tr.im/n0Cy']\n",
            "['http://tweet.sg']\n",
            "['http://plurk.com/p/xt8cm']\n",
            "['http://twitpic.com/5ddsi']\n",
            "['http://bit.ly/2mFB2']\n",
            "['http://myloc.me/41Kq']\n",
            "['http://bit.ly/icbfj']\n",
            "['http://plurk.com/p/wy2m1']\n",
            "['http://twitpic.com/7lqs3']\n",
            "['http://myloc.me/teY']\n",
            "['http://tinyurl.com/qx38my']\n",
            "['http://tinyurl.com/le9d4f']\n",
            "['http://bit.ly/9ougy']\n",
            "['www.myspace.com/tynishakeli']\n",
            "['http://twitpic.com/6iytx']\n",
            "['http://myloc.me/4rxt']\n",
            "['http://www.dryjuly.com']\n",
            "['http://bit.ly/W6fxT']\n",
            "['http://plurk.com/p/z3z6p']\n",
            "['nbc.com']\n",
            "['www.iamsoannoyed.com']\n",
            "['http://ur1.ca/5b4m']\n",
            "['http://twitpic.com/7h3dq']\n",
            "['http://snurl.com/feo4p']\n",
            "['http://is.gd/16lr2']\n",
            "['bomb.com']\n",
            "['http://twitpic.com/6a28u']\n",
            "['http://twitpic.com/6iyah']\n",
            "['http://bit.ly/IQPPD']\n",
            "['http://twitpic.com/6fsl4']\n",
            "['http://twitpic.com/8cin5']\n",
            "['http://nightmoves.me']\n",
            "['http://twitpic.com/7rwwa']\n",
            "['http://bit.ly/Bfy9B']\n",
            "['http://bit.ly/16Z4xZ']\n",
            "['http://twitpic.com/4gzhk']\n",
            "['http://foamslidefactory.blogspot.com']\n",
            "['http://tinyurl.com/cp5yhr']\n",
            "['http://tinyurl.com/n7wk2x']\n",
            "['http://tinyurl.com/pemwuh']\n",
            "['myspace.com', 'http://twurl.nl/8tb0wf']\n",
            "['http://bit.ly/5JFuk']\n",
            "['http://plurk.com/p/xnsr2']\n",
            "['http://flickr.com/labelsphotography']\n",
            "['http://bit.ly/WdBZk']\n",
            "['http://twitpic.com/5esg1']\n",
            "['http://twitpic.com/6tnnx']\n",
            "['http://bit.ly/DmvFY']\n",
            "['www.bondno9.com']\n",
            "['http://twitpic.com/6hs1b']\n",
            "['http://twitpic.com/7uis6']\n",
            "['http://twitpic.com/5cqrn']\n",
            "['http://blip.fm/~7rhzx']\n",
            "['http://tinyurl.com/c6d3mv']\n",
            "['http://is.gd/JKhP']\n",
            "['simpletownUSA.com']\n",
            "['http://blip.fm/~60p98']\n",
            "['http://www.moteldemoka.com']\n",
            "['http://bit.ly/ozTuO']\n",
            "['bit.ly', 'http://bit.ly/15yZMq']\n",
            "['http://yfrog.com/0y7wcvj']\n",
            "['http://overheardinwow.wordpress.com']\n",
            "['www.m2e.asia']\n",
            "['http://bittenbybooks.com/?p']\n",
            "['http://twitpic.com/7ogps']\n",
            "['http://www.spiritisup.com/ahuginyourinboxgmb.html']\n",
            "['www.myspace.com/mdadinosaur']\n",
            "['http://twitpic.com/6p6ae']\n",
            "['http://twitpic.com/6v7qi']\n",
            "['http://blip.fm/~7at6t']\n",
            "['http://twitpic.com/7cnge']\n",
            "['http://blip.fm/~68tu1']\n",
            "['http://twitpic.com/6bqli']\n",
            "['http://bit.ly/ZsnZf']\n",
            "['http://tinyurl.com/dkpvt7']\n",
            "['http://bit.ly/19UgRP']\n",
            "['soompi.com']\n",
            "['http://plurk.com/p/xf7i8']\n",
            "['http://www.carolinamusicawards.com']\n",
            "['http://twitpic.com/6p0rw']\n",
            "['http://www.facebook.com/banpei']\n",
            "['http://twitpic.com/3kygb']\n",
            "['http://emonky.deviantart.com', 'www.flickr.com/emonky']\n",
            "['http://tinyurl.com/djgyf7']\n",
            "['http://tinyurl.com/25giveaway']\n",
            "['http://tinyurl.com/ZeniGeba']\n",
            "['http://bit.ly/blizzconticket09']\n",
            "['http://bit.ly/14xvke']\n",
            "['http://twitpic.com/6d2kp']\n",
            "['www.tweeterfollow.com']\n",
            "['http://www.myspace.com/ofmachinestheband']\n",
            "['http://blip.fm/~4mwmi']\n",
            "['http://plurk.com/p/rr5fj']\n",
            "['http://tinyurl.com/ndnb75']\n",
            "['http://twitpic.com/2t5nz']\n",
            "['http://twitpic.com/69ffo']\n",
            "['happalong.com']\n",
            "['http://tr.im/m4Hs']\n",
            "['http://kl.am/Uln']\n",
            "['http://myloc.me/29e6']\n",
            "['http://twitpic.com/6om7s']\n",
            "['http://is.gd/RmKw']\n",
            "['www.tweeteradder.com']\n",
            "['http://tinyurl.com/ry9wap']\n",
            "['http://apps.facebook.com/catbook/profile/view/620328']\n",
            "['http://blip.fm/~7nyu0']\n",
            "['http://bit.ly/BPP4d', 'http://bit.ly/gtEB']\n",
            "['http://tweet.sg']\n",
            "['http://plurk.com/p/z2laq']\n",
            "['http://in.groups.yahoo.com/group/TheComicsClub']\n",
            "['http://twitpic.com/6qdq6']\n",
            "['http://mypict.me/54O0']\n",
            "['http://plurk.com/p/xdjio']\n",
            "['http://plurk.com/p/1100lw']\n",
            "['http://twitpic.com/66580']\n",
            "['http://myloc.me/1XIz']\n",
            "['http://twitpic.com/6g0m7']\n",
            "['http://www.modelhomeproject.com']\n",
            "['http://blip.fm/~6st7k']\n",
            "['http://bit.ly/7l9s1']\n",
            "['http://bit.ly/ZCYEE']\n",
            "['http://twitpic.com/7tvhv']\n",
            "['http://twitpic.com/7h5bf']\n",
            "['http://bit.ly/dB3Tx', 'http://bit.ly/13Jtir']\n",
            "['http://bit.ly/RElFH']\n",
            "['http://bit.ly/FVFwq']\n",
            "['http://tinyurl.com/mrp4x6']\n",
            "['http://twitpic.com/7g6v1']\n",
            "['http://plurk.com/p/110kmy']\n",
            "['http://ustre.am/3pO8']\n",
            "['http://twitpic.com/7mt6v']\n",
            "['www.disneycollegeprogram.com']\n",
            "['http://plurk.com/p/rjw8t']\n",
            "['http://twitpic.com/6onox']\n",
            "['http://twitpic.com/69ey9']\n",
            "['http://bit.ly/7rL9S']\n",
            "['http://pau27figureskater.multiply.com']\n",
            "['http://blip.fm/~7rfdl']\n",
            "['http://tinyurl.com/cs73el']\n",
            "['http://tinyurl.com/ku9yks']\n",
            "['http://twitpic.com/80023']\n",
            "['http://mypict.me/QR7']\n",
            "['www.stringbeancoffeeshop.com']\n",
            "['http://blip.fm/~7aytk']\n",
            "['http://tr.im/nqmj']\n",
            "['http://www.hakkastudy.in.th']\n"
          ]
        }
      ],
      "source": [
        "for i in df_data[df_data['URLs'].apply(lambda x: len(x) > 0)]['URLs'].to_list():\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jENr0Gqos6H"
      },
      "source": [
        "## f. Count the number of tweets for each day of the week. Eg Mon: 58, Tues: 20, Wed..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1lPRxi9pR1t"
      },
      "outputs": [],
      "source": [
        "def getDay(sentence):\n",
        "  return re.findall(\"Fri|Mon|Thu|Sun|Tue|Wed|Sat\", sentence)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dswe26lFmQi5"
      },
      "outputs": [],
      "source": [
        "df_data['Day'] = df_data['DATE_TIME'].apply(lambda x: getDay(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUuFLwxkqgqk",
        "outputId": "c85ced00-76f0-4e27-d429-61eafb57646d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sun    1328\n",
              "Mon     872\n",
              "Fri     864\n",
              "Sat     417\n",
              "Wed     299\n",
              "Tue     286\n",
              "Thu     221\n",
              "Name: Day, dtype: int64"
            ]
          },
          "execution_count": 615,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data['Day'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YQY-xmesj5X"
      },
      "source": [
        "# B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-itGBwas5XQ"
      },
      "source": [
        "## a. Total number of occurrences of the given word and sentences containing that word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCtBlSzwqiYl"
      },
      "outputs": [],
      "source": [
        "def re_find_word_in_sentence(word, sentence):\n",
        "  return len(re.findall(f'\\\\b{word}\\\\b', sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHLgD9f4tfmK"
      },
      "outputs": [],
      "source": [
        "def find_word_counts(df_data, word, class_label):\n",
        "  filtered_df = df_data[df_data['LABEL'] == class_label]\n",
        "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_word_in_sentence(word, x))\n",
        "  total_occurence = sum(filtered_df['Counts'])\n",
        "  sentences_containing_word = filtered_df[filtered_df['Counts'] > 0]['TEXT'].to_list()\n",
        "  return total_occurence, sentences_containing_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBTpIOtcticy",
        "outputId": "f4bdbdbf-9a5e-4927-8070-0ac1307c9b6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "total_occurence, sentences_containing_word = find_word_counts(df_data, 'i', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmWK4oTPts86",
        "outputId": "048479e4-abdb-4c91-ff15-f4d14d904e69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "450"
            ]
          },
          "execution_count": 645,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_occurence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REgyf0cnvEoa",
        "outputId": "5041960d-7124-4800-c039-ecc3c49218f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"@buckhollywood I Cant Watch That i'm in the UK  Can you tell me what its about? Please x\",\n",
              " '@mykiaisosm omj ur bad and mean i should not have meet u in 2nd grade even thouggh we hated each other i should have stayed like that ',\n",
              " ' i missed the game',\n",
              " \"dr office... hopefully finding out why i've been so sick  it's so hard to keep my eyes open\",\n",
              " 'i feel like death...my next investment?going to the spa! i need a new body that can function  (via @IngaDurgin)i herd sleep is good 4 dat']"
            ]
          },
          "execution_count": 647,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences_containing_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx1eiKQNvZID"
      },
      "source": [
        "## b. Number of sentences starting with the given word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7f0n-rmvFeQ"
      },
      "outputs": [],
      "source": [
        "def re_find_sentence_starting_with_word(word, sentence):\n",
        "  if re.findall(f'^\\s*{word}\\\\b', sentence):\n",
        "    return 1\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohoOqq48wwA9",
        "outputId": "04378213-307e-40cd-b870-9c2adb72bae5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "execution_count": 688,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re_find_sentence_starting_with_word(\"i\", \" am i \"), re_find_sentence_starting_with_word(\"i\", \" i am i \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4qf7D0tvjwr"
      },
      "outputs": [],
      "source": [
        "def find_all_sentences_starting_with_word(df_data, word, class_label):\n",
        "  filtered_df = df_data[df_data['LABEL'] == class_label]\n",
        "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_sentence_starting_with_word(word, x))\n",
        "  total_occurence = sum(filtered_df['Counts'])\n",
        "  sentences_starting_with_word = filtered_df[filtered_df['Counts'] == 1]['TEXT'].to_list()\n",
        "  return total_occurence, sentences_starting_with_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldBGVY_evmvH",
        "outputId": "dd9a0064-1eb6-4f95-f6d2-bfa91ac55e22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "total_occurence, sentences_starting_with_word = find_all_sentences_starting_with_word(df_data, 'i', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3H49wwTwY48",
        "outputId": "ebda4a54-f32f-411c-a06b-1fff1085e02a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2,\n",
              " [' i missed the game',\n",
              "  \" i don't know, darlin. I'm so sorry. I wish i had what you need to make everything right.\"])"
            ]
          },
          "execution_count": 682,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_occurence, sentences_starting_with_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxu11MKXwi_I"
      },
      "source": [
        "## c. Number of sentences ending with the given word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58BVCGSnwguC"
      },
      "outputs": [],
      "source": [
        "def re_find_sentence_ending_with_word(word, sentence):\n",
        "  if re.findall(f'\\\\b{word}\\s*$', sentence):\n",
        "    return 1\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpfAFnRRw3J3",
        "outputId": "9ca64ae0-c694-4811-c6c4-8b208bef7831"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "execution_count": 693,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re_find_sentence_ending_with_word(\"i\", \" am\"), re_find_sentence_ending_with_word(\"i\", \" i am i\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB9e_reFw9ds"
      },
      "outputs": [],
      "source": [
        "def find_all_sentences_ending_with_word(df_data, word, class_label):\n",
        "  filtered_df = df_data[df_data['LABEL'] == class_label]\n",
        "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_sentence_ending_with_word(word, x))\n",
        "  total_occurence = sum(filtered_df['Counts'])\n",
        "  sentences_ending_with_word = filtered_df[filtered_df['Counts'] == 1]['TEXT'].to_list()\n",
        "  return total_occurence, sentences_ending_with_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQbh03oJxNgA",
        "outputId": "ca165aad-2375-4d41-98ba-f8ddb38b6e95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "total_occurence, sentences_ending_with_word = find_all_sentences_ending_with_word(df_data, 'scared', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIYOTYfTxQAn",
        "outputId": "dc4265ee-64aa-4e6b-9815-d17f9c83c31f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, ['About to get threaded and scared '])"
            ]
          },
          "execution_count": 698,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_occurence, sentences_ending_with_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dzbkQusxRlQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
