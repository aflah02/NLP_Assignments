{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud -q\n",
    "!pip install autocorrect -q\n",
    "!pip install nltk\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "xURHYz8XsLoR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "k-ptEnijslOo",
    "outputId": "7077eda8-1c04-4a89-f17e-ea5b286477a5"
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('A1_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "RBquCbrZsny5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri Jun 05 14:26:50 2009</td>\n",
       "      <td>About to get threaded and scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thu May 14 10:13:55 2009</td>\n",
       "      <td>@awaisnaseer I like Shezan Mangooo too!!! I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri Jun 05 21:02:20 2009</td>\n",
       "      <td>worked on my car after work. showering then go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jun 14 22:25:52 2009</td>\n",
       "      <td>@Marama Actually we start this afternoon!  I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun May 31 00:42:12 2009</td>\n",
       "      <td>@gfalcone601 Aww Gi.don't worry.we'll vote for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>1</td>\n",
       "      <td>Sat Jun 06 22:45:26 2009</td>\n",
       "      <td>@QandQ My performances on my CLEP tests.  #qshock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>0</td>\n",
       "      <td>Tue Jun 16 10:17:07 2009</td>\n",
       "      <td>ugh no, rcn had all the true blood episodes on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri May 01 22:00:42 2009</td>\n",
       "      <td>Just returned from the forest! Sarah (my merch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jun 07 02:09:46 2009</td>\n",
       "      <td>is proud of her dad and his piece of work. ( h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri May 22 04:49:37 2009</td>\n",
       "      <td>Just woke up, gonna eat pizza for breakfast. A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4287 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL                 DATE_TIME  \\\n",
       "0         0  Fri Jun 05 14:26:50 2009   \n",
       "1         1  Thu May 14 10:13:55 2009   \n",
       "2         1  Fri Jun 05 21:02:20 2009   \n",
       "3         1  Sun Jun 14 22:25:52 2009   \n",
       "4         1  Sun May 31 00:42:12 2009   \n",
       "...     ...                       ...   \n",
       "4282      1  Sat Jun 06 22:45:26 2009   \n",
       "4283      0  Tue Jun 16 10:17:07 2009   \n",
       "4284      1  Fri May 01 22:00:42 2009   \n",
       "4285      1  Sun Jun 07 02:09:46 2009   \n",
       "4286      0  Fri May 22 04:49:37 2009   \n",
       "\n",
       "                                                   TEXT  \n",
       "0                     About to get threaded and scared   \n",
       "1     @awaisnaseer I like Shezan Mangooo too!!! I ha...  \n",
       "2     worked on my car after work. showering then go...  \n",
       "3     @Marama Actually we start this afternoon!  I w...  \n",
       "4     @gfalcone601 Aww Gi.don't worry.we'll vote for...  \n",
       "...                                                 ...  \n",
       "4282  @QandQ My performances on my CLEP tests.  #qshock  \n",
       "4283  ugh no, rcn had all the true blood episodes on...  \n",
       "4284  Just returned from the forest! Sarah (my merch...  \n",
       "4285  is proud of her dad and his piece of work. ( h...  \n",
       "4286  Just woke up, gonna eat pizza for breakfast. A...  \n",
       "\n",
       "[4287 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "woQssMfKAQe3"
   },
   "outputs": [],
   "source": [
    "from customregexes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAVPGC_8rRM7"
   },
   "source": [
    "# I. REGULAR EXPRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n87jQodrSJ5"
   },
   "source": [
    "# A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtiD3IRnsvR0"
   },
   "source": [
    "## a. Average number of sentences and tokens \n",
    "- ! or ? (Even Continous)\n",
    "- . Followed by one or more spaces and then a capital character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "1-38IM7hfmVc"
   },
   "outputs": [],
   "source": [
    "df_data['sentence_count'] = df_data['TEXT'].apply(lambda x: findSentenceCount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "0    1.800500\n",
       "1    1.854832\n",
       "Name: sentence_count, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(['LABEL'])['sentence_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "GiDosJOaRAgw"
   },
   "outputs": [],
   "source": [
    "df_data['token_count'] = df_data['TEXT'].apply(lambda x: findTokenCount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['tokens'] = df_data['TEXT'].apply(lambda x: findTokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXZd-DZcsu_4",
    "outputId": "0d516035-5bdb-4aab-fc08-f6d6d3ba0f8a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "0    14.811500\n",
       "1    14.139921\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(['LABEL'])['token_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Total number of words starting with consonants and vowels\n",
    "- Words starting with either Consonant or Vowel (depending on subpart) followed by either an alphabet, an accented alphabet, an apostrophe or a hyphen.\n",
    "- This was done to ensure no cases of say spanish text which creep in twitter data from countries like the U.S. are not missed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "21bourgDKKyL"
   },
   "outputs": [],
   "source": [
    "df_data['words_starting_with_vowel'] = df_data['TEXT'].apply(lambda x: countWordsStartingWithVowel(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIUx_RhlqJkQ",
    "outputId": "1220f13f-7b0f-4796-eda8-53e5c875cfd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14177"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['words_starting_with_vowel'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "0    6989\n",
       "1    7188\n",
       "Name: words_starting_with_vowel, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(['LABEL'])['words_starting_with_vowel'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "wibwwaouCJDC"
   },
   "outputs": [],
   "source": [
    "df_data['words_starting_with_consonant'] = df_data['TEXT'].apply(lambda x: countWordsStartingWithConsonant(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ookPUBYcqHY8",
    "outputId": "f60386ca-600a-4767-8c56-79dc666d52b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37196"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['words_starting_with_consonant'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "0    17995\n",
       "1    19201\n",
       "Name: words_starting_with_consonant, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(['LABEL'])['words_starting_with_consonant'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYRKw0v3Pq2n"
   },
   "source": [
    "## c. Lowercase the text and report the number of unique tokens present before and after lower casing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique Tokens Before Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Fa4UbCdpPOoL"
   },
   "outputs": [],
   "source": [
    "def get_unique_tokens(tokens):\n",
    "    unique_tokens = set()\n",
    "    for token_list in tokens:\n",
    "        for token in token_list:\n",
    "            unique_tokens.add(token)\n",
    "    return len(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_data['tokens'].to_list()\n",
    "df_data['lowercase_text'] = df_data['TEXT'].apply(lambda x: lowercase(x))\n",
    "df_data['lowercase_tokens'] = df_data['lowercase_text'].apply(lambda x: findTokens(x))\n",
    "lowercase_tokens = df_data['lowercase_tokens'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens 13167\n",
      "Unique Lowercase Tokens 11565\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Tokens\", get_unique_tokens(tokens))\n",
    "print(\"Unique Lowercase Tokens\", get_unique_tokens(lowercase_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_LABEL_0 = df_data[df_data['LABEL'] == 0]['tokens'].to_list()\n",
    "lowercase_tokens_LABEL_0 = df_data[df_data['LABEL'] == 0]['lowercase_tokens'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens 6991\n",
      "Unique Lowercase Tokens 6215\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Tokens\", get_unique_tokens(tokens_LABEL_0))\n",
    "print(\"Unique Lowercase Tokens\", get_unique_tokens(lowercase_tokens_LABEL_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_LABEL_1 = df_data[df_data['LABEL'] == 1]['tokens'].to_list()\n",
    "lowercase_tokens_LABEL_1 = df_data[df_data['LABEL'] == 1]['lowercase_tokens'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens 8622\n",
      "Unique Lowercase Tokens 7615\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Tokens\", get_unique_tokens(tokens_LABEL_1))\n",
    "print(\"Unique Lowercase Tokens\", get_unique_tokens(lowercase_tokens_LABEL_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdc4Ufy2XiVE"
   },
   "source": [
    "## d. Count and list all the usernames.\n",
    "\n",
    "Basic Rules and Assumptions:\n",
    "\n",
    "- According to [Twitter Guidelines](https://help.twitter.com/en/managing-your-account/twitter-username-rules)\n",
    "  - Your username cannot be longer than 15 characters. Your name can be longer (50 characters) or shorter than 4 characters, but usernames are kept shorter for the sake of ease.\n",
    "  - A username can only contain alphanumeric characters (letters A-Z, numbers 0-9) with the exception of underscores, as noted above. Check to make sure your desired username doesn't contain any symbols, dashes, or spaces.\n",
    "  - *Optional Rule to Spot Users* - Usernames containing the words Twitter or Admin cannot be claimed. No account names can contain Twitter or Admin unless they are official Twitter accounts.\n",
    "\n",
    "- Some experimentation with our Twitter Handle helped us reach the following conclusions:\n",
    "  - @UserName can't be placed with other alphanumeric characters so abc@user 9@user are not valid and won't tag the user\n",
    "  - X@UserName where X is a punctuation is valid but the following cases also don't allow tagging - \n",
    "    - @@xyz\n",
    "    - _@xyz\n",
    "\n",
    "Hence we propose regexes which actually find real tag matches instead of say a user just writing an @ somewhere in the tweet and it getting falsely matched as a tagged user when it really isn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "QxPV1weuX1rn"
   },
   "outputs": [],
   "source": [
    "df_data['UserNames'] = df_data['TEXT'].apply(lambda x: findUsernames(x))\n",
    "df_data['UserNamesCounts'] = df_data['TEXT'].apply(lambda x: findUsernameCount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIRVTSpZqmDL",
    "outputId": "9d11b68b-019f-4731-c556-66324d14d6c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2108"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['UserNamesCounts'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username Count Label 0 -  803\n"
     ]
    }
   ],
   "source": [
    "print(\"Username Count Label 0 - \" , df_data[df_data['LABEL'] == 0]['UserNamesCounts'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username Count Label 1 -  1305\n"
     ]
    }
   ],
   "source": [
    "print(\"Username Count Label 1 - \" , df_data[df_data['LABEL'] == 1]['UserNamesCounts'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "1IQi6p5SqsHy"
   },
   "outputs": [],
   "source": [
    "usernames = df_data['UserNames'].to_numpy()\n",
    "flattened_usernames = []\n",
    "for i in usernames:\n",
    "  flattened_usernames.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "We06hPGvrYVB",
    "outputId": "3ec7df54-41ab-4cde-8bf6-2aa8f5bc7cea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(flattened_usernames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9D-7fwqgYNe"
   },
   "source": [
    "## e. Count and list all the urls\n",
    "\n",
    "Basic Rules and Assumptions\n",
    "\n",
    "- `http`, `https`, `www` common starters for URLs usually\n",
    "- Manually investigating all sentences which contain `http` shows no false positives that is all occurences of `http` correspond to links\n",
    "- Manually investigating all sentences which contain `www` shows lots of false positives due to words like `aww`\n",
    "- On manual inspection we find no sentences which contain `https`\n",
    "- According to [Twitter's official Blog](https://help.twitter.com/en/using-twitter/url-shortener), Twitter uses a URL-Shortener which converts links to the form `t.co` however there are no positive matches for this in our dataset. The only matches that arise are spurious matches in links like `blogspot.com`\n",
    "- A URL maybe as simple as `www.xyz.abc` and as complex as `http://www.xyz.abc/efg` and can get even more complex by adding / to index more indepth into pages\n",
    "- There are false positives that we encounter such as `Gi.don` or `worry.we` but they are both valid URLs as well as there can be custom domains by those names. We assume the domain will be atleast 2 characters long to account for `.me`, `.uk` etc.\n",
    "- Numbers have been allowed as [only numbers can form valid URLs](https://stackoverflow.com/q/56804936/13858953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Pzjw7gUbgj3H"
   },
   "outputs": [],
   "source": [
    "df_data['URLs'] = df_data['TEXT'].apply(lambda x: findURLs(x))\n",
    "df_data['URLCounts'] = df_data['TEXT'].apply(lambda x: findURLCount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Count Label 0 -  60\n"
     ]
    }
   ],
   "source": [
    "print(\"URL Count Label 0 - \" , df_data[df_data['LABEL'] == 0]['URLCounts'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Count Label 1 -  145\n"
     ]
    }
   ],
   "source": [
    "print(\"URL Count Label 1 - \" , df_data[df_data['LABEL'] == 1]['URLCounts'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jENr0Gqos6H"
   },
   "source": [
    "## f. Count the number of tweets for each day of the week. Eg Mon: 58, Tues: 20, Wed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Dswe26lFmQi5"
   },
   "outputs": [],
   "source": [
    "df_data['Day'] = df_data['DATE_TIME'].apply(lambda x: getDay(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day Counts Label 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sun    565\n",
       "Fri    473\n",
       "Mon    391\n",
       "Thu    171\n",
       "Tue    154\n",
       "Wed    127\n",
       "Sat    119\n",
       "Name: Day, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Day Counts Label 0\")\n",
    "df_data[df_data['LABEL'] == 0]['Day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUuFLwxkqgqk",
    "outputId": "c85ced00-76f0-4e27-d429-61eafb57646d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day Counts Label 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sun    763\n",
       "Mon    481\n",
       "Fri    391\n",
       "Sat    298\n",
       "Wed    172\n",
       "Tue    132\n",
       "Thu     50\n",
       "Name: Day, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Day Counts Label 1\")\n",
    "df_data[df_data['LABEL'] == 1]['Day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv('Save_post_regular_exp_section.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YQY-xmesj5X"
   },
   "source": [
    "# B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-itGBwas5XQ"
   },
   "source": [
    "## a. Total number of occurrences of the given word and sentences containing that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "kCtBlSzwqiYl"
   },
   "outputs": [],
   "source": [
    "def re_find_word_in_sentence(word, sentence):\n",
    "  return len(re.findall(f'\\\\b{word}\\\\b', sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "PHLgD9f4tfmK"
   },
   "outputs": [],
   "source": [
    "def find_word_counts(df_data, word, class_label):\n",
    "  filtered_df = df_data[df_data['LABEL'] == class_label]\n",
    "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_word_in_sentence(word, x))\n",
    "  total_occurence = sum(filtered_df['Counts'])\n",
    "  sentences_containing_word = filtered_df[filtered_df['Counts'] > 0]['TEXT'].to_list()\n",
    "  return total_occurence, sentences_containing_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBTpIOtcticy",
    "outputId": "f4bdbdbf-9a5e-4927-8070-0ac1307c9b6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3216\\4136765138.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_word_in_sentence(word, x))\n"
     ]
    }
   ],
   "source": [
    "total_occurence, sentences_containing_word = find_word_counts(df_data, 'i', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmWK4oTPts86",
    "outputId": "048479e4-abdb-4c91-ff15-f4d14d904e69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REgyf0cnvEoa",
    "outputId": "5041960d-7124-4800-c039-ecc3c49218f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@buckhollywood I Cant Watch That i'm in the UK  Can you tell me what its about? Please x\",\n",
       " '@mykiaisosm omj ur bad and mean i should not have meet u in 2nd grade even thouggh we hated each other i should have stayed like that ',\n",
       " ' i missed the game',\n",
       " \"dr office... hopefully finding out why i've been so sick  it's so hard to keep my eyes open\",\n",
       " 'i feel like death...my next investment?going to the spa! i need a new body that can function  (via @IngaDurgin)i herd sleep is good 4 dat']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_containing_word[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cx1eiKQNvZID"
   },
   "source": [
    "## b. Number of sentences starting with the given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "C7f0n-rmvFeQ"
   },
   "outputs": [],
   "source": [
    "def re_find_sentence_starting_with_word(word, sentence):\n",
    "  if re.findall(f'^\\s*{word}\\\\b', sentence):\n",
    "    return 1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohoOqq48wwA9",
    "outputId": "04378213-307e-40cd-b870-9c2adb72bae5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_find_sentence_starting_with_word(\"i\", \" am i \"), re_find_sentence_starting_with_word(\"i\", \" i am i \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "B4qf7D0tvjwr"
   },
   "outputs": [],
   "source": [
    "def find_all_sentences_starting_with_word(df_data, word, class_label):\n",
    "  filtered_df = df_data[df_data['LABEL'] == class_label]\n",
    "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_sentence_starting_with_word(word, x))\n",
    "  total_occurence = sum(filtered_df['Counts'])\n",
    "  sentences_starting_with_word = filtered_df[filtered_df['Counts'] == 1]['TEXT'].to_list()\n",
    "  return total_occurence, sentences_starting_with_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldBGVY_evmvH",
    "outputId": "dd9a0064-1eb6-4f95-f6d2-bfa91ac55e22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3216\\110637590.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_sentence_starting_with_word(word, x))\n"
     ]
    }
   ],
   "source": [
    "total_occurence, sentences_starting_with_word = find_all_sentences_starting_with_word(df_data, 'i', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "a3H49wwTwY48",
    "outputId": "ebda4a54-f32f-411c-a06b-1fff1085e02a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52,\n",
       " [' i missed the game',\n",
       "  'i feel like death...my next investment?going to the spa! i need a new body that can function  (via @IngaDurgin)i herd sleep is good 4 dat',\n",
       "  \"i bet i was mistaking.. nah i'm not surprised \",\n",
       "  'i swear i just felt a earthquake  lol',\n",
       "  'i need to go out soon   i dont wanna but this weight aint gonna shift its self is it lol x',\n",
       "  \"i'm eating chocolate covered pretzels which is reminding me of Mallrats and making me not want to eat them anymore. \",\n",
       "  'i really miss photoshop ',\n",
       "  'i hate it im not yet done to my homework!! ',\n",
       "  \"i have officially lost all feeling in my legs! playing the sims 3 for seven hours isn't good... \",\n",
       "  \"i'm gonna miss those dayss \",\n",
       "  'i miss my feather duster ',\n",
       "  'i wish i was watching The Hills ',\n",
       "  'i miss @romylovesmcfly, anke and paula  i was going with them to tokio hotel.',\n",
       "  'i feel really bad i just talked to my parents like they were my slaves, and they were being so nice to me  punish me.',\n",
       "  \"i #blamedrewscancer for my b'day falling on a glorious sunny day this year &amp; me being cooped up in the office all day \",\n",
       "  'i hate being sick.&amp;in the same week as all my half yearlys, ahh poo ',\n",
       "  \"i can't make up my mind on what to wear \",\n",
       "  \"i has a headache.  i'm home alone doin stuff til 3-4 then partyin it upp!\",\n",
       "  \"i can't wait to play in leeds on my birthday, i wish i was on tour though! \",\n",
       "  'i dont want to pack! ',\n",
       "  'i have to leave ',\n",
       "  'i know i told him i didnt want him to stay home with me but i lied, i really did want him to stay home with me ',\n",
       "  \" i don't know, darlin. I'm so sorry. I wish i had what you need to make everything right.\",\n",
       "  'i think spoon is sick ',\n",
       "  \"i am watching the insider live and i sawed miley cyrus break up with nick......ummm isn't that old news? im just angry \",\n",
       "  \"i think i'm getting sick \",\n",
       "  'i dont want rudy to die oh my god i am going to cry why did death say that was going to happen ',\n",
       "  \"i miss my best friend!   we're one and the same &lt;3\",\n",
       "  'i wish our internet wasnt fucked. i could be doing any number of things right now ',\n",
       "  \"i hope THEY didn't return together \",\n",
       "  'i miss my crush ',\n",
       "  'i feel kind of sick.  hopefully supernatural will make me feel better.',\n",
       "  'i am cold ',\n",
       "  'i firmly believe the worst part of the working day is waiting for public transport - so boring. ',\n",
       "  \"i'm tired. and feel slightly ill. \",\n",
       "  'i dunno y @tweetpeete speaks of himself thru 3rd party tweets lol but @tweetpeete is mad cuz sports is finna suck til sunday ',\n",
       "  \"i'm so bored! i wanna go out with my friends.. \",\n",
       "  'i need to do my religion assignment, but its at school ',\n",
       "  \"i'm Bumbed about the lego batman being sold out \",\n",
       "  'i have to fill two hours ',\n",
       "  'i want my brother home so i can play sims 3 ',\n",
       "  'i didnt read cuz i felt to yucky so i had a yogut and that just made made me feel more yucky..damn teeth  and now i dont know what to eat',\n",
       "  'i dont want to start studying again today ',\n",
       "  'i blew up this balloon that tasted and smelt like burnt rubber and now i have a fricken headache    ',\n",
       "  'i am hungry but I ate so many bagels today i can feel them coming up. I needz a cheeseburger! ',\n",
       "  'i should be in malia right now! ',\n",
       "  \"i'm gunna cry when asher &amp; kay kiss in fame  better yet, i'll close my eyes.\",\n",
       "  'i fucking miss him so much ',\n",
       "  'i am about 99% sure that i just killed the new digital camera my parents got me yesterday ',\n",
       "  \"i'm sooo tired but i just can't seem to get to sleep! \",\n",
       "  'i am sad ',\n",
       "  'i wonder if there any anything as GIT tract transplant. i want to replace all my intestines to a healthier one '])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_occurence, sentences_starting_with_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxu11MKXwi_I"
   },
   "source": [
    "## c. Number of sentences ending with the given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "58BVCGSnwguC"
   },
   "outputs": [],
   "source": [
    "def re_find_sentence_ending_with_word(word, sentence):\n",
    "  if re.findall(f'\\\\b{word}\\s*$', sentence):\n",
    "    return 1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpfAFnRRw3J3",
    "outputId": "9ca64ae0-c694-4811-c6c4-8b208bef7831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_find_sentence_ending_with_word(\"i\", \" am\"), re_find_sentence_ending_with_word(\"i\", \" i am i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "vB9e_reFw9ds"
   },
   "outputs": [],
   "source": [
    "def find_all_sentences_ending_with_word(df_data, word, class_label):\n",
    "  filtered_df = df_data[df_data['LABEL'] == class_label]\n",
    "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_sentence_ending_with_word(word, x))\n",
    "  total_occurence = sum(filtered_df['Counts'])\n",
    "  sentences_ending_with_word = filtered_df[filtered_df['Counts'] == 1]['TEXT'].to_list()\n",
    "  return total_occurence, sentences_ending_with_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQbh03oJxNgA",
    "outputId": "ca165aad-2375-4d41-98ba-f8ddb38b6e95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3216\\981797926.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Counts'] = filtered_df['TEXT'].apply(lambda x: re_find_sentence_ending_with_word(word, x))\n"
     ]
    }
   ],
   "source": [
    "total_occurence, sentences_ending_with_word = find_all_sentences_ending_with_word(df_data, 'scared', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIYOTYfTxQAn",
    "outputId": "dc4265ee-64aa-4e6b-9815-d17f9c83c31f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, ['About to get threaded and scared '])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_occurence, sentences_ending_with_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('A1_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tokenization(text):\n",
    "    twt_tk = TweetTokenizer()\n",
    "    return twt_tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wordnet_words = []\n",
    "\n",
    "for word in wn.words():\n",
    "    wordnet_words.append(word)\n",
    "\n",
    "words = set(wordnet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Spelling correction using jaccard distance\n",
    "def spelling_correction_jaccard(text):\n",
    "    res = []\n",
    "    for curr in text:\n",
    "        if curr not in words:\n",
    "            crrct_words = []\n",
    "            for w in words:\n",
    "                crrct_words.append([jaccard_distance(set(w), set(curr)), w])\n",
    "            crrct_words = sorted(crrct_words, key = lambda x: x[0])\n",
    "            res.append(crrct_words[0][1][0])\n",
    "        else:\n",
    "            res.append(curr)\n",
    "    return res\n",
    "\n",
    "# Spelling correction using edit distance\n",
    "def spelling_correction_edit(text):\n",
    "    res = []\n",
    "    for curr in text:\n",
    "        if curr not in words:\n",
    "            crrct_words = []\n",
    "            for w in words:\n",
    "                crrct_words.append([edit_distance(w, curr), w])\n",
    "            crrct_words = sorted(crrct_words, key = lambda x: x[0])\n",
    "            res.append(crrct_words[0][1])\n",
    "        else:\n",
    "            res.append(curr)\n",
    "    return res\n",
    "\n",
    "# Spelling correction using autocorrect\n",
    "def spelling_correction_autocorrect(text):\n",
    "    spell = Speller()\n",
    "    res = []\n",
    "    for curr in text:\n",
    "        if curr not in words:\n",
    "            res.append(spell(curr))\n",
    "        else:\n",
    "            res.append(curr)\n",
    "    return res\n",
    "\n",
    "def spelling_correction(text, method ='autocorrect'):\n",
    "    if method == 'autocorrect':\n",
    "        return spelling_correction_autocorrect(text)\n",
    "    elif method == 'edit_distance':\n",
    "        return spelling_correction_edit(text)\n",
    "    elif method == 'jaccard':\n",
    "        return spelling_correction_jaccard(text)\n",
    "    else:\n",
    "        raise ValueError('Please use a valid method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we can either do lemmatization or stemming, doing both at the same time will not be useful\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def lemmatization_text(text):\n",
    "    wn = WordNetLemmatizer()\n",
    "    lemmatized_text = []\n",
    "    for each in text:\n",
    "        lemmatized_word = wn.lemmatize(each)\n",
    "        lemmatized_text.append(lemmatized_word)\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def stemming_text(text):\n",
    "    stemmed_text = []\n",
    "    ps = PorterStemmer()\n",
    "    for each in text:\n",
    "        stemmed_word = ps.stem(each)\n",
    "        stemmed_text.append(stemmed_word)\n",
    "    return ' '.join(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    puncts = string.punctuation\n",
    "    s = \"\"\n",
    "    for i in text:\n",
    "        if i not in puncts:\n",
    "            s += i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.append(\"i'm\")\n",
    "def remove_stopwords(text):\n",
    "    pattern = re.compile(r'\\b(' + (r'|'.join(stop_words)) + r')\\b\\s*')\n",
    "    return pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespaces(text):\n",
    "    return re.sub(r'\\s*\\s', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_html(text):\n",
    "    urls = findURLs(text)\n",
    "    for i in urls:\n",
    "        text = re.sub(f\"{i}\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_users(text):\n",
    "    username = findUsernames(text)\n",
    "    for i in username:\n",
    "        text = re.sub(f\"{i}\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    return lowercase(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we prefer to perform lemmatization over stemming so that the words don't lose their inherent meaning\n",
    "# and we perform spelling correction using the autocorrect libaray over edit_distance and jaccard because jaccard\n",
    "# and edit_distance are much slower and inaccurate in places\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = lowercase_text(text)\n",
    "    text = remove_url_html(text)\n",
    "    text = remove_users(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_whitespaces(text)\n",
    "    text = tokenization(text)\n",
    "    text = spelling_correction(text)\n",
    "    text = lemmatization_text(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text_verbose(text):\n",
    "    print(\"Recieved input ->\", text)\n",
    "    text = lowercase_text(text)\n",
    "    print(\"After lowercasing text ->\", text)\n",
    "    text = remove_url_html(text)\n",
    "    print(\"After removing urls and html ->\", text)\n",
    "    text = remove_users(text)\n",
    "    print(\"After removing usernames ->\", text)\n",
    "    text = remove_stopwords(text)\n",
    "    print(\"After removing stopwords ->\", text)\n",
    "    text = remove_punctuations(text)\n",
    "    print(\"After removing punctuations ->\", text)\n",
    "    text = remove_whitespaces(text)\n",
    "    print(\"After removing extra whitespaces ->\", text)\n",
    "    text = tokenization(text)\n",
    "    print(\"After tokenization ->\", text)\n",
    "    text = spelling_correction(text)\n",
    "    print(\"After spelling correction ->\", text)\n",
    "    text = lemmatization_text(text)\n",
    "    print(\"After lemmatization ->\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083ecaf123324dccbd19e78568c61fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [162]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREPROCESSED_TEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEXT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Input \u001b[1;32mIn [162]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREPROCESSED_TEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [161]\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m remove_whitespaces(text)\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenization(text)\n\u001b[1;32m---> 13\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mspelling_correction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m text \u001b[38;5;241m=\u001b[39m lemmatization_text(text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Input \u001b[1;32mIn [136]\u001b[0m, in \u001b[0;36mspelling_correction\u001b[1;34m(text, method)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspelling_correction\u001b[39m(text, method \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautocorrect\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautocorrect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling_correction_autocorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medit_distance\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m spelling_correction_edit(text)\n",
      "Input \u001b[1;32mIn [136]\u001b[0m, in \u001b[0;36mspelling_correction_autocorrect\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspelling_correction_autocorrect\u001b[39m(text):\n\u001b[1;32m---> 34\u001b[0m     spell \u001b[38;5;241m=\u001b[39m \u001b[43mSpeller\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m curr \u001b[38;5;129;01min\u001b[39;00m text:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\autocorrect\\__init__.py:83\u001b[0m, in \u001b[0;36mSpeller.__init__\u001b[1;34m(self, lang, threshold, nlp_data, fast, only_replacements)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m lang\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_tar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m nlp_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nlp_data\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast \u001b[38;5;241m=\u001b[39m fast\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_replacements \u001b[38;5;241m=\u001b[39m only_replacements\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\autocorrect\\__init__.py:74\u001b[0m, in \u001b[0;36mload_from_tar\u001b[1;34m(lang, file_name)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m closing(tarfile\u001b[38;5;241m.\u001b[39mopen(archive_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr:gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m tarf:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m closing(tarf\u001b[38;5;241m.\u001b[39mextractfile(file_name)) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_data[\"PREPROCESSED_TEXT\"] = df_data[\"TEXT\"].progress_apply(lambda row: preprocess_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv('PreProcessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label 0\n",
    "\n",
    "sample_0 = \"Bill Killed.  http://tr.im/nqmj  I'm sure I'm not the first to make this joke...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recieved input -> Bill Killed.  http://tr.im/nqmj  I'm sure I'm not the first to make this joke...\n",
      "After lowercasing text -> bill killed.  http://tr.im/nqmj  i'm sure i'm not the first to make this joke...\n",
      "After removing urls and html -> bill killed.    i'm sure i'm not the first to make this joke...\n",
      "After removing usernames -> bill killed.    i'm sure i'm not the first to make this joke...\n",
      "After removing stopwords -> bill killed.    'sure 'first make joke...\n",
      "After removing punctuations -> bill killed    sure first make joke\n",
      "After removing extra whitespaces -> bill killed sure first make joke\n",
      "After tokenization -> ['bill', 'killed', 'sure', 'first', 'make', 'joke']\n",
      "After spelling correction -> ['bill', 'killed', 'sure', 'first', 'make', 'joke']\n",
      "After lemmatization -> bill killed sure first make joke\n"
     ]
    }
   ],
   "source": [
    "preprocess_text_verbose(sample_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label 1\n",
    "\n",
    "sample_1 = 'is proud of her dad and his piece of work. ( http://www.hakkastudy.in.th/) keep it up papa '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recieved input -> is proud of her dad and his piece of work. ( http://www.hakkastudy.in.th/) keep it up papa \n",
      "After lowercasing text -> is proud of her dad and his piece of work. ( http://www.hakkastudy.in.th/) keep it up papa \n",
      "After removing urls and html -> is proud of her dad and his piece of work. ( /) keep it up papa \n",
      "After removing usernames -> is proud of her dad and his piece of work. ( /) keep it up papa \n",
      "After removing stopwords -> proud dad piece work. ( /) keep papa \n",
      "After removing punctuations -> proud dad piece work   keep papa \n",
      "After removing extra whitespaces -> proud dad piece work keep papa \n",
      "After tokenization -> ['proud', 'dad', 'piece', 'work', 'keep', 'papa']\n",
      "After spelling correction -> ['proud', 'dad', 'piece', 'work', 'keep', 'papa']\n",
      "After lemmatization -> proud dad piece work keep papa\n"
     ]
    }
   ],
   "source": [
    "preprocess_text_verbose(sample_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
