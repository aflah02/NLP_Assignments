{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocess_text import *\n",
    "from bigrams import *\n",
    "from smoothers import *\n",
    "from LanguageModels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('A1_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['TEXT'].to_list()\n",
    "def preprocess_text(text):\n",
    "    text = lowercase_text(text)\n",
    "    text = remove_url_html(text)\n",
    "    text = remove_users(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_whitespaces(text)\n",
    "    text = tokenization(text)\n",
    "    text = spelling_correction(text, 'textblob')\n",
    "    return text\n",
    "df['preprocessed_text'] = df['TEXT'].apply(preprocess_text)\n",
    "preprocess_text = df['preprocessed_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = {}\n",
    "for sentence in preprocess_text:\n",
    "    for word in sentence:\n",
    "        if word in unigram_counts:\n",
    "            unigram_counts[word] += 1\n",
    "        else:\n",
    "            unigram_counts[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8496/8496 [00:00<00:00, 57774.06it/s]\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "ls_word_sentiment_vader = []\n",
    "for word in tqdm(unigram_counts):\n",
    "    ls_word_sentiment_vader.append((word, sid.polarity_scores(word)['compound']))\n",
    "\n",
    "with open('ls_word_sentiment_vader.pickle', 'wb') as f:\n",
    "    pickle.dump(ls_word_sentiment_vader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "hf_sentiment_model = pipeline('sentiment-analysis')\n",
    "ls_word_sentiment_hf = []\n",
    "for word in tqdm(unigram_counts):\n",
    "    hf_res = hf_sentiment_model(word)\n",
    "    score = hf_res[0]['score']\n",
    "    pos_neg = hf_res[0]['label']\n",
    "    if pos_neg == 'NEGATIVE':\n",
    "        score = -score\n",
    "    ls_word_sentiment_hf.append((word,score))\n",
    "\n",
    "with open('ls_word_sentiment_vader.pickle', 'wb') as f:\n",
    "    pickle.dump(ls_word_sentiment_vader, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_wrapper = Bigrams(preprocess_text)\n",
    "# bigrams = bigram_wrapper.get_bigrams()\n",
    "# vocab_len = bigram_wrapper.vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoother = LMSmoothers(bigrams, vocab_len)\n",
    "# laplace_smoothed_bigrams = smoother.laplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('laplace_smoothed_bigrams.pickle', 'wb') as handle:\n",
    "#     pickle.dump(laplace_smoothed_bigrams, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle file\n",
    "import pickle\n",
    "with open('laplace_smoothed_bigrams.pickle', 'rb') as handle:\n",
    "    laplace_smoothed_bigrams = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigramLM = LanguageModel(laplace_smoothed_bigrams, unigram_counts, 'vader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\NLP_Assignments\\A2\\A2.ipynb Cell 10\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/NLP_Assignments/A2/A2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentence_pos \u001b[39m=\u001b[39m BigramLM\u001b[39m.\u001b[39;49mgenerate_text([\u001b[39m'\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mam\u001b[39;49m\u001b[39m'\u001b[39;49m], sentiment\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, length\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/NLP_Assignments/A2/A2.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sentence_neg \u001b[39m=\u001b[39m BigramLM\u001b[39m.\u001b[39mgenerate_text([\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mam\u001b[39m\u001b[39m'\u001b[39m], sentiment\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, length\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m)\n",
      "File \u001b[1;32md:\\Desktop\\NLP_Assignments\\A2\\LanguageModels.py:24\u001b[0m, in \u001b[0;36mLanguageModel.generate_text\u001b[1;34m(self, prompt, sentiment, length)\u001b[0m\n\u001b[0;32m     22\u001b[0m length \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(prompt)\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length):\n\u001b[1;32m---> 24\u001b[0m     next_word \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_next_word(last_word, sentiment)\n\u001b[0;32m     25\u001b[0m     prompt\u001b[39m.\u001b[39mappend(next_word)\n\u001b[0;32m     26\u001b[0m     last_word \u001b[39m=\u001b[39m next_word\n",
      "File \u001b[1;32md:\\Desktop\\NLP_Assignments\\A2\\LanguageModels.py:37\u001b[0m, in \u001b[0;36mLanguageModel.get_next_word\u001b[1;34m(self, word, sentiment)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m bigram, freq \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbigrams\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentiment_model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvader\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 37\u001b[0m         normalized_prob \u001b[39m=\u001b[39m freq \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munigram_counts[bigram[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m sentiment \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msid\u001b[39m.\u001b[39;49mpolarity_scores(bigram[\u001b[39m1\u001b[39;49m])[\u001b[39m'\u001b[39m\u001b[39mcompound\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     38\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentiment_model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhf\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     39\u001b[0m         normalized_prob \u001b[39m=\u001b[39m freq \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munigram_counts[bigram[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m sentiment \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhf_sentiment_model(bigram[\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\sentiment\\vader.py:361\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mReturn a float for sentiment strength based on the input text.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[39mPositive values are positive valence, negative value are negative\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[39mvalence.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m sentitext \u001b[39m=\u001b[39m SentiText(\n\u001b[0;32m    362\u001b[0m     text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstants\u001b[39m.\u001b[39;49mPUNC_LIST, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstants\u001b[39m.\u001b[39;49mREGEX_REMOVE_PUNCTUATION\n\u001b[0;32m    363\u001b[0m )\n\u001b[0;32m    364\u001b[0m sentiments \u001b[39m=\u001b[39m []\n\u001b[0;32m    365\u001b[0m words_and_emoticons \u001b[39m=\u001b[39m sentitext\u001b[39m.\u001b[39mwords_and_emoticons\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\sentiment\\vader.py:277\u001b[0m, in \u001b[0;36mSentiText.__init__\u001b[1;34m(self, text, punc_list, regex_remove_punctuation)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords_and_emoticons \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words_and_emoticons()\n\u001b[0;32m    275\u001b[0m \u001b[39m# doesn't separate words from\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m# adjacent punctuation (keeps emoticons & contractions)\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cap_diff \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallcap_differential(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwords_and_emoticons)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\sentiment\\vader.py:323\u001b[0m, in \u001b[0;36mSentiText.allcap_differential\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    321\u001b[0m allcap_words \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m--> 323\u001b[0m     \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39;49misupper():\n\u001b[0;32m    324\u001b[0m         allcap_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    325\u001b[0m cap_differential \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words) \u001b[39m-\u001b[39m allcap_words\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentence_pos = BigramLM.generate_text(['i', 'am'], sentiment=1, length=7)\n",
    "sentence_neg = BigramLM.generate_text(['i', 'am'], sentiment=-1, length=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'swamped', 'so', 'much', 'love', 'you', 'nonstop', 'coz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'swamped', 'so', 'much', 'love', 'you', 'nonstop', 'coz']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6034315.198668093"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BigramLM.computePerplexity(sentence_pos), BigramLM.computePerplexity(sentence_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4404"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(\"good\")['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
