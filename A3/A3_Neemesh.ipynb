{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0tw0xe31hT59",
      "metadata": {
        "id": "0tw0xe31hT59"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers -q\n",
        "!pip install gensim -q\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Kyode\\clg\\NLP_Assignments\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "import string\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import gensim.downloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [],
      "source": [
        "INPUT_PATH = 'stsbenchmark/'\n",
        "\n",
        "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"sts-\"+ dataset_type + \".csv\"\n",
        "  \"\"\"\n",
        "  Take the input path and return the dataframe\n",
        "  \"\"\"\n",
        "  # Open File as Text File\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    # Read the file as a list of lines\n",
        "    lines = f.readlines()\n",
        "\n",
        "  output = []\n",
        "  for line in lines:\n",
        "    # Split the line by tab\n",
        "    line = line.strip().split('\\t')\n",
        "    # Append the line to output\n",
        "    output.append(line[:len(columns)])\n",
        "\n",
        "  # Convert the output to a dataframe\n",
        "  df = pd.DataFrame(output, columns=columns)\n",
        "  return df\n",
        "\n",
        "# df_<dataset_type> = read_sts_csv(dataset_type) # create the train, dev and test dataframes\n",
        "df_train = read_sts_csv(\"train\")\n",
        "df_dev = read_sts_csv(\"dev\")\n",
        "df_test = read_sts_csv(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1df60664",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>type</th>\n",
              "      <th>year</th>\n",
              "      <th>id</th>\n",
              "      <th>score</th>\n",
              "      <th>sent_a</th>\n",
              "      <th>sent_b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>0001</td>\n",
              "      <td>5.000</td>\n",
              "      <td>A plane is taking off.</td>\n",
              "      <td>An air plane is taking off.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>0004</td>\n",
              "      <td>3.800</td>\n",
              "      <td>A man is playing a large flute.</td>\n",
              "      <td>A man is playing a flute.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>0005</td>\n",
              "      <td>3.800</td>\n",
              "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
              "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>0006</td>\n",
              "      <td>2.600</td>\n",
              "      <td>Three men are playing chess.</td>\n",
              "      <td>Two men are playing chess.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>0009</td>\n",
              "      <td>4.250</td>\n",
              "      <td>A man is playing the cello.</td>\n",
              "      <td>A man seated is playing the cello.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5744</th>\n",
              "      <td>main-news</td>\n",
              "      <td>headlines</td>\n",
              "      <td>2016</td>\n",
              "      <td>1456</td>\n",
              "      <td>0</td>\n",
              "      <td>Severe Gales As Storm Clodagh Hits Britain</td>\n",
              "      <td>Merkel pledges NATO solidarity with Latvia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5745</th>\n",
              "      <td>main-news</td>\n",
              "      <td>headlines</td>\n",
              "      <td>2016</td>\n",
              "      <td>1465</td>\n",
              "      <td>0</td>\n",
              "      <td>Dozens of Egyptians hostages taken by Libyan t...</td>\n",
              "      <td>Egyptian boat crash death toll rises as more b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5746</th>\n",
              "      <td>main-news</td>\n",
              "      <td>headlines</td>\n",
              "      <td>2016</td>\n",
              "      <td>1466</td>\n",
              "      <td>0</td>\n",
              "      <td>President heading to Bahrain</td>\n",
              "      <td>President Xi: China to continue help to fight ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5747</th>\n",
              "      <td>main-news</td>\n",
              "      <td>headlines</td>\n",
              "      <td>2016</td>\n",
              "      <td>1470</td>\n",
              "      <td>0</td>\n",
              "      <td>China, India vow to further bilateral ties</td>\n",
              "      <td>China Scrambles to Reassure Jittery Stock Traders</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5748</th>\n",
              "      <td>main-news</td>\n",
              "      <td>headlines</td>\n",
              "      <td>2016</td>\n",
              "      <td>1492</td>\n",
              "      <td>0</td>\n",
              "      <td>Putin spokesman: Doping charges appear unfounded</td>\n",
              "      <td>The Latest on Severe Weather: 1 Dead in Texas ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5749 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             source       type      year    id  score  \\\n",
              "0     main-captions     MSRvid  2012test  0001  5.000   \n",
              "1     main-captions     MSRvid  2012test  0004  3.800   \n",
              "2     main-captions     MSRvid  2012test  0005  3.800   \n",
              "3     main-captions     MSRvid  2012test  0006  2.600   \n",
              "4     main-captions     MSRvid  2012test  0009  4.250   \n",
              "...             ...        ...       ...   ...    ...   \n",
              "5744      main-news  headlines      2016  1456      0   \n",
              "5745      main-news  headlines      2016  1465      0   \n",
              "5746      main-news  headlines      2016  1466      0   \n",
              "5747      main-news  headlines      2016  1470      0   \n",
              "5748      main-news  headlines      2016  1492      0   \n",
              "\n",
              "                                                 sent_a  \\\n",
              "0                                A plane is taking off.   \n",
              "1                       A man is playing a large flute.   \n",
              "2         A man is spreading shreded cheese on a pizza.   \n",
              "3                          Three men are playing chess.   \n",
              "4                           A man is playing the cello.   \n",
              "...                                                 ...   \n",
              "5744         Severe Gales As Storm Clodagh Hits Britain   \n",
              "5745  Dozens of Egyptians hostages taken by Libyan t...   \n",
              "5746                       President heading to Bahrain   \n",
              "5747         China, India vow to further bilateral ties   \n",
              "5748   Putin spokesman: Doping charges appear unfounded   \n",
              "\n",
              "                                                 sent_b  \n",
              "0                           An air plane is taking off.  \n",
              "1                             A man is playing a flute.  \n",
              "2     A man is spreading shredded cheese on an uncoo...  \n",
              "3                            Two men are playing chess.  \n",
              "4                    A man seated is playing the cello.  \n",
              "...                                                 ...  \n",
              "5744         Merkel pledges NATO solidarity with Latvia  \n",
              "5745  Egyptian boat crash death toll rises as more b...  \n",
              "5746  President Xi: China to continue help to fight ...  \n",
              "5747  China Scrambles to Reassure Jittery Stock Traders  \n",
              "5748  The Latest on Severe Weather: 1 Dead in Texas ...  \n",
              "\n",
              "[5749 rows x 7 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [],
      "source": [
        "NON_CONEXTUAL_MODEL_TYPE = 'fasttext-wiki-news-subwords-300'\n",
        "CONEXTUAL_MODEL_TYPE = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL = \"sentence-transformers/all-mpnet-base-v2\" # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "INPUT_PATH = 'stsbenchmark/'\n",
        "BATCH_SIZE = 32\n",
        "OUT_DIM_DENSE = 556\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well.\n",
        "NUM_WARMUP = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Train Set 0.35\n",
            "For Dev Set 0.178\n",
            "For Test Set 0.263\n"
          ]
        }
      ],
      "source": [
        "def get_sentence_vector(sentence, model):\n",
        "    # Preprocess the sentence\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    # Remove punctuation\n",
        "    words = [word.strip(string.punctuation) for word in words]\n",
        "    vector = np.zeros(len(model[0]))\n",
        "    count = 0\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            vector += model[word]\n",
        "            count += 1\n",
        "    # Return the average of the vectors\n",
        "    return vector / count\n",
        "\n",
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "\n",
        "  sent_a = data_frame['sent_a'].values\n",
        "  sent_b = data_frame['sent_b'].values\n",
        "\n",
        "  sent_a_vectors = np.zeros((len(sent_a), 300))\n",
        "  sent_b_vectors = np.zeros((len(sent_b), 300))\n",
        "\n",
        "  for i in range(len(sent_a)):\n",
        "    sent_a_vectors[i] = get_sentence_vector(sent_a[i], non_cont_model1)\n",
        "    sent_b_vectors[i] = get_sentence_vector(sent_b[i], non_cont_model1)\n",
        "\n",
        "  return sent_a_vectors, sent_b_vectors\n",
        "  \n",
        "# non_cont_model1 = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model1(data_frame)\n",
        "# feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
        "# feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
        "# feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
        "\n",
        "# Save the features to disk\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_train.npy', feature_1_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_train.npy', feature_2_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_dev.npy', feature_1_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_dev.npy', feature_2_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_test.npy', feature_1_test)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_test.npy', feature_2_test)\n",
        "\n",
        "# Load the features from disk\n",
        "feature_1_train = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_train.npy')\n",
        "feature_2_train = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_train.npy')\n",
        "feature_1_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_dev.npy')\n",
        "feature_2_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_dev.npy')\n",
        "feature_1_test = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_test.npy')\n",
        "feature_2_test = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_test.npy')\n",
        "\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Combine by mean\n",
        "X_train, Y_train = np.mean([feature_1_train, feature_2_train], axis=0), df_train['score'].values\n",
        "X_dev, Y_dev = np.mean([feature_1_dev, feature_2_dev], axis=0), df_dev['score'].values\n",
        "X_test, Y_test = np.mean([feature_1_test, feature_2_test], axis=0), df_test['score'].values\n",
        "\n",
        "# Initiate a regression model and train it.\n",
        "regression_model = linear_model.Ridge(alpha=0.5)\n",
        "regression_model.fit(X_train, Y_train)\n",
        "\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets.\n",
        "print(\"For Train Set\", round(stats.spearmanr(regression_model.predict(X_train), Y_train)[0], 3))\n",
        "print(\"For Dev Set\", round(stats.spearmanr(regression_model.predict(X_dev), Y_dev)[0], 3))\n",
        "print(\"For Test Set\", round(stats.spearmanr(regression_model.predict(X_test), Y_test)[0], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Train Set 0.491\n",
            "For Dev Set 0.173\n",
            "For Test Set 0.254\n"
          ]
        }
      ],
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  sent_a = data_frame['sent_a'].values\n",
        "  sent_b = data_frame['sent_b'].values\n",
        "\n",
        "  sent_a_vectors = non_cont_model2.encode(sent_a)\n",
        "  sent_b_vectors = non_cont_model2.encode(sent_b)\n",
        "\n",
        "  return sent_a_vectors, sent_b_vectors\n",
        "\n",
        "# non_cont_model2 = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "# feature_1_train, feature_2_train = get_feature_model2(df_train)\n",
        "# feature_1_dev, feature_2_dev = get_feature_model2(df_dev)\n",
        "# feature_1_test, feature_2_test = get_feature_model2(df_test)\n",
        "\n",
        "# Save the features to disk\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_train.npy', feature_1_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_train.npy', feature_2_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_dev.npy', feature_1_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_dev.npy', feature_2_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_test.npy', feature_1_test)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_test.npy', feature_2_test)\n",
        "\n",
        "# Load the features from disk\n",
        "feature_1_train = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_train.npy')\n",
        "feature_2_train = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_train.npy')\n",
        "feature_1_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_dev.npy')\n",
        "feature_2_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_dev.npy')\n",
        "feature_1_test = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_test.npy')\n",
        "feature_2_test = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_test.npy')\n",
        "\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "X_train, Y_train = np.mean([feature_1_train, feature_2_train], axis=0), df_train['score'].values\n",
        "X_dev, Y_dev = np.mean([feature_1_dev, feature_2_dev], axis=0), df_dev['score'].values\n",
        "X_test, Y_test = np.mean([feature_1_test, feature_2_test], axis=0), df_test['score'].values\n",
        "\n",
        "# Initiate a regression model and train it.\n",
        "regression_model = linear_model.Ridge(alpha=0.5)\n",
        "regression_model.fit(X_train, Y_train)\n",
        "\n",
        "# Print spearman correlation on the predicted output of the dev and test sets.\n",
        "print(\"For Train Set\", round(stats.spearmanr(regression_model.predict(X_train), Y_train)[0], 3))\n",
        "print(\"For Dev Set\", round(stats.spearmanr(regression_model.predict(X_dev), Y_dev)[0], 3))\n",
        "print(\"For Test Set\", round(stats.spearmanr(regression_model.predict(X_test), Y_test)[0], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 180/180 [49:19<00:00, 16.44s/it]\n",
            "Iteration: 100%|██████████| 180/180 [50:24<00:00, 16.80s/it]\n",
            "Epoch: 100%|██████████| 2/2 [1:39:44<00:00, 2992.13s/it]\n"
          ]
        }
      ],
      "source": [
        "def form_data(data_frame):\n",
        "    \"\"\"\n",
        "    Input a data frame and return the dataloder.\n",
        "    \"\"\"\n",
        "    sent_a_samples = data_frame[\"sent_a\"].values\n",
        "    sent_b_samples = data_frame[\"sent_b\"].values\n",
        "    labels = data_frame[\"score\"].values.astype(float)\n",
        "    labels /= 5\n",
        "\n",
        "    labels = torch.from_numpy(labels).float()\n",
        "\n",
        "    examples = []\n",
        "    for i in range(sent_a_samples.shape[0]):\n",
        "        examples.append(InputExample(\n",
        "                            texts = [sent_a_samples[i], sent_b_samples[i]],\n",
        "                            label = labels[i]\n",
        "                        ))\n",
        "\n",
        "    dataloader = DataLoader(examples, shuffle = True, batch_size = BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "  \"\"\"\n",
        "  Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "  \"\"\"\n",
        "\n",
        "dataloader_train = form_data(df_train)\n",
        "dataloader_dev = form_data(df_dev)\n",
        "dataloader_test = form_data(df_test)\n",
        "\n",
        "base_model = models.Transformer(HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL)\n",
        "layer_pooling = models.Pooling(base_model.get_word_embedding_dimension())\n",
        "layer_dense = models.Dense(in_features = layer_pooling.get_sentence_embedding_dimension(), out_features = OUT_DIM_DENSE)\n",
        "model3 = SentenceTransformer(modules = [base_model, layer_pooling, layer_dense])\n",
        "loss = losses.CosineSimilarityLoss(model3)\n",
        "\n",
        "# evaluator = EmbeddingSimilarityEvaluator(df_dev[\"sent_a\"].values, df_dev[\"sent_b\"].values, df_dev[\"score\"].values.astype(float)/5)\n",
        "\n",
        "# Fit the model3.\n",
        "model3.fit(train_objectives = [(dataloader_train, loss)], epochs = NUM_EPOCHS, warmup_steps = NUM_WARMUP)\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "612c0042",
      "metadata": {},
      "outputs": [],
      "source": [
        "x1_train, x2_train = model3.encode(df_train[\"sent_a\"].values), model3.encode(df_train[\"sent_b\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "86c384bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Train Set 0.929\n"
          ]
        }
      ],
      "source": [
        "print(\"For Train Set\", round(stats.spearmanr(util.cos_sim(x1_train, x2_train).diagonal(), df_train[\"score\"].values)[0], 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6161e825",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Dev Set 0.899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Kyode\\clg\\NLP_Assignments\\venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
            "  warnings.warn(\"The input array could not be properly \"\n"
          ]
        }
      ],
      "source": [
        "x1_dev, x2_dev = model3.encode(df_dev[\"sent_a\"].values), model3.encode(df_dev[\"sent_b\"].values)\n",
        "print(\"For Dev Set\", round(stats.spearmanr(util.cos_sim(x1_dev, x2_dev).diagonal(), df_dev[\"score\"].values)[0], 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9305d23b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Test Set 0.863\n"
          ]
        }
      ],
      "source": [
        "x1_test, x2_test = model3.encode(df_test[\"sent_a\"].values), model3.encode(df_test[\"sent_b\"].values)\n",
        "print(\"For Test Set\", round(stats.spearmanr(util.cos_sim(x1_test, x2_test).diagonal(), df_test[\"score\"].values)[0], 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1de445",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "7c8e19a14fe58e373378a0fdbcb9aefd23f8dfb00ff30ccb3e8933e0b75542ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
