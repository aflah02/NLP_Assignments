{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0tw0xe31hT59",
      "metadata": {
        "id": "0tw0xe31hT59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
            "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers -q\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [],
      "source": [
        "INPUT_PATH = 'stsbenchmark/'\n",
        "\n",
        "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"sts-\"+ dataset_type + \".csv\"\n",
        "  \"\"\"\n",
        "  Take the input path and return the dataframe\n",
        "  \"\"\"\n",
        "  # Open File as Text File\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    # Read the file as a list of lines\n",
        "    lines = f.readlines()\n",
        "\n",
        "  output = []\n",
        "  for line in lines:\n",
        "    # Split the line by tab\n",
        "    line = line.strip().split('\\t')\n",
        "    # Append the line to output\n",
        "    output.append(line[:len(columns)])\n",
        "\n",
        "  # Convert the output to a dataframe\n",
        "  df = pd.DataFrame(output, columns=columns)\n",
        "  return df\n",
        "\n",
        "# df_<dataset_type> = read_sts_csv(dataset_type) # create the train, dev and test dataframes\n",
        "df_train = read_sts_csv(\"train\")\n",
        "df_dev = read_sts_csv(\"dev\")\n",
        "df_test = read_sts_csv(\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1877320520.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Input \u001b[1;32mIn [4]\u001b[1;36m\u001b[0m\n\u001b[1;33m    NON_CONEXTUAL_MODEL_TYPE = <NON_CONEXTUAL_MODEL_TYPE>\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "NON_CONEXTUAL_MODEL_TYPE = <NON_CONEXTUAL_MODEL_TYPE>\n",
        "CONEXTUAL_MODEL_TYPE = <CONEXTUAL_MODEL_TYPE>\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL = <HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL> # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "INPUT_PATH = <INPUT_FOLDER_PATH>\n",
        "BATCH_SIZE = <BATCH SIZE>\n",
        "OUT_DIM_DENSE = <OUT_DIM_DENSE>\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [],
      "source": [
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "\n",
        "# non_cont_model1 = \n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model1(data_frame)\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Initiate a regression model and train it.\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [],
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "\n",
        "# non_cont_model2 = \n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Initiate a regression model and train it.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [],
      "source": [
        "def form_data(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the dataloder.\n",
        "  \"\"\"\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "  \"\"\"\n",
        "  Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "  \"\"\"\n",
        "\n",
        "# dataloader_<dataset_type> = form_data(data_frame)\n",
        "# base_model = \n",
        "# layer_ppoling = \n",
        "# layer_dense = \n",
        "# model3 = \n",
        "# loss =\n",
        "\n",
        "# Fit the model3.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
