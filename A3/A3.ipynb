{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0tw0xe31hT59",
      "metadata": {
        "id": "0tw0xe31hT59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: torchvision in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (0.13.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (1.1.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (1.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (0.9.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (1.20.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sentence_transformers) (4.21.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk->sentence_transformers) (8.0.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.10)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
            "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [],
      "source": [
        "INPUT_PATH = 'stsbenchmark/'\n",
        "\n",
        "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"sts-\"+ dataset_type + \".csv\"\n",
        "  \"\"\"\n",
        "  Take the input path and return the dataframe\n",
        "  \"\"\"\n",
        "  # Open File as Text File\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    # Read the file as a list of lines\n",
        "    lines = f.readlines()\n",
        "\n",
        "  output = []\n",
        "  for line in lines:\n",
        "    # Split the line by tab\n",
        "    line = line.strip().split('\\t')\n",
        "    # Append the line to output\n",
        "    output.append(line[:len(columns)])\n",
        "\n",
        "  # Convert the output to a dataframe\n",
        "  df = pd.DataFrame(output, columns=columns)\n",
        "  return df\n",
        "\n",
        "# df_<dataset_type> = read_sts_csv(dataset_type) # create the train, dev and test dataframes\n",
        "df_train = read_sts_csv(\"train\")\n",
        "df_dev = read_sts_csv(\"dev\")\n",
        "df_test = read_sts_csv(\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [],
      "source": [
        "NON_CONEXTUAL_MODEL_TYPE = <NON_CONEXTUAL_MODEL_TYPE>\n",
        "CONEXTUAL_MODEL_TYPE = <CONEXTUAL_MODEL_TYPE>\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL = <HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL> # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "INPUT_PATH = <INPUT_FOLDER_PATH>\n",
        "BATCH_SIZE = <BATCH SIZE>\n",
        "OUT_DIM_DENSE = <OUT_DIM_DENSE>\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [],
      "source": [
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "\n",
        "# non_cont_model1 = \n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model1(data_frame)\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Initiate a regression model and train it.\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [],
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "\n",
        "# non_cont_model2 = \n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Initiate a regression model and train it.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [],
      "source": [
        "def form_data(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the dataloder.\n",
        "  \"\"\"\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "  \"\"\"\n",
        "  Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "  \"\"\"\n",
        "\n",
        "# dataloader_<dataset_type> = form_data(data_frame)\n",
        "# base_model = \n",
        "# layer_ppoling = \n",
        "# layer_dense = \n",
        "# model3 = \n",
        "# loss =\n",
        "\n",
        "# Fit the model3.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
