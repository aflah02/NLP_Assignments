{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0tw0xe31hT59",
      "metadata": {
        "id": "0tw0xe31hT59"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers -q\n",
        "!pip install gensim -q\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Kyode\\clg\\NLP_Assignments\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "import string\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import gensim.downloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [],
      "source": [
        "INPUT_PATH = 'stsbenchmark/'\n",
        "\n",
        "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"sts-\"+ dataset_type + \".csv\"\n",
        "  \"\"\"\n",
        "  Take the input path and return the dataframe\n",
        "  \"\"\"\n",
        "  # Open File as Text File\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    # Read the file as a list of lines\n",
        "    lines = f.readlines()\n",
        "\n",
        "  output = []\n",
        "  for line in lines:\n",
        "    # Split the line by tab\n",
        "    line = line.strip().split('\\t')\n",
        "    # Append the line to output, discarding extra columns which occur in some rows\n",
        "    output.append(line[:len(columns)])\n",
        "\n",
        "  # Convert the output to a dataframe\n",
        "  df = pd.DataFrame(output, columns=columns)\n",
        "  return df\n",
        "\n",
        "# df_<dataset_type> = read_sts_csv(dataset_type) # create the train, dev and test dataframes\n",
        "df_train = read_sts_csv(\"train\")\n",
        "df_dev = read_sts_csv(\"dev\")\n",
        "df_test = read_sts_csv(\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [],
      "source": [
        "NON_CONEXTUAL_MODEL_TYPE = 'fasttext-wiki-news-subwords-300'\n",
        "CONEXTUAL_MODEL_TYPE = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL = \"sentence-transformers/all-mpnet-base-v2\" # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "INPUT_PATH = 'stsbenchmark/'\n",
        "BATCH_SIZE = 32\n",
        "OUT_DIM_DENSE = 556\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well.\n",
        "NUM_WARMUP = 500\n",
        "MODEL_SAVE_PATH = \"model/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Train Set 0.35\n",
            "For Dev Set 0.178\n",
            "For Test Set 0.263\n"
          ]
        }
      ],
      "source": [
        "def get_sentence_vector(sentence, model):\n",
        "    # Preprocess the sentence\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    # Remove punctuation\n",
        "    words = [word.strip(string.punctuation) for word in words]\n",
        "    # Create a vector of zeros that has the same length as the output of the model\n",
        "    vector = np.zeros(len(model[0]))\n",
        "    count = 0\n",
        "    # Loop through each word, adding the vector for that word to the total, if the word is not in the model, ignore it\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            vector += model[word]\n",
        "            count += 1\n",
        "    # Return the average of the vectors (the count is the number of words in the sentence that were in the model)\n",
        "    return vector / count\n",
        "\n",
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "\n",
        "  sent_a = data_frame['sent_a'].values\n",
        "  sent_b = data_frame['sent_b'].values\n",
        "\n",
        "  sent_a_vectors = np.zeros((len(sent_a), 300))\n",
        "  sent_b_vectors = np.zeros((len(sent_b), 300))\n",
        "\n",
        "  for i in range(len(sent_a)):\n",
        "    sent_a_vectors[i] = get_sentence_vector(sent_a[i], non_cont_model1)\n",
        "    sent_b_vectors[i] = get_sentence_vector(sent_b[i], non_cont_model1)\n",
        "\n",
        "  return sent_a_vectors, sent_b_vectors\n",
        "  \n",
        "# non_cont_model1 = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model1(data_frame)\n",
        "# feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
        "# feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
        "# feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
        "\n",
        "# Save the features to disk\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_train.npy', feature_1_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_train.npy', feature_2_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_dev.npy', feature_1_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_dev.npy', feature_2_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_test.npy', feature_1_test)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_test.npy', feature_2_test)\n",
        "\n",
        "# Load the features from disk\n",
        "feature_1_train = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_train.npy')\n",
        "feature_2_train = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_train.npy')\n",
        "feature_1_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_dev.npy')\n",
        "feature_2_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_dev.npy')\n",
        "feature_1_test = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_1_test.npy')\n",
        "feature_2_test = np.load('WordEmbeddingSaves/' + 'non_cont_model1' + '_feature_2_test.npy')\n",
        "\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Combine by mean\n",
        "X_train, Y_train = np.mean([feature_1_train, feature_2_train], axis=0), df_train['score'].values\n",
        "X_dev, Y_dev = np.mean([feature_1_dev, feature_2_dev], axis=0), df_dev['score'].values\n",
        "X_test, Y_test = np.mean([feature_1_test, feature_2_test], axis=0), df_test['score'].values\n",
        "\n",
        "# Initiate a regression model and train it.\n",
        "regression_model = linear_model.Ridge(alpha=0.5)\n",
        "regression_model.fit(X_train, Y_train)\n",
        "\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets.\n",
        "print(\"For Train Set\", round(stats.spearmanr(regression_model.predict(X_train), Y_train)[0], 3))\n",
        "print(\"For Dev Set\", round(stats.spearmanr(regression_model.predict(X_dev), Y_dev)[0], 3))\n",
        "print(\"For Test Set\", round(stats.spearmanr(regression_model.predict(X_test), Y_test)[0], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Train Set 0.491\n",
            "For Dev Set 0.173\n",
            "For Test Set 0.254\n"
          ]
        }
      ],
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  sent_a = data_frame['sent_a'].values\n",
        "  sent_b = data_frame['sent_b'].values\n",
        "\n",
        "  sent_a_vectors = non_cont_model2.encode(sent_a)\n",
        "  sent_b_vectors = non_cont_model2.encode(sent_b)\n",
        "\n",
        "  return sent_a_vectors, sent_b_vectors\n",
        "\n",
        "## After Testing Some Models, we found that this model is the best for our task/\n",
        "# non_cont_model2 = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "# feature_1_train, feature_2_train = get_feature_model2(df_train)\n",
        "# feature_1_dev, feature_2_dev = get_feature_model2(df_dev)\n",
        "# feature_1_test, feature_2_test = get_feature_model2(df_test)\n",
        "\n",
        "# Save the features to disk\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_train.npy', feature_1_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_train.npy', feature_2_train)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_dev.npy', feature_1_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_dev.npy', feature_2_dev)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_test.npy', feature_1_test)\n",
        "# np.save('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_test.npy', feature_2_test)\n",
        "\n",
        "# Load the features from disk\n",
        "feature_1_train = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_train.npy')\n",
        "feature_2_train = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_train.npy')\n",
        "feature_1_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_dev.npy')\n",
        "feature_2_dev = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_dev.npy')\n",
        "feature_1_test = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_1_test.npy')\n",
        "feature_2_test = np.load('WordEmbeddingSaves/' + 'non_cont_model2' + '_feature_2_test.npy')\n",
        "\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "X_train, Y_train = np.mean([feature_1_train, feature_2_train], axis=0), df_train['score'].values\n",
        "X_dev, Y_dev = np.mean([feature_1_dev, feature_2_dev], axis=0), df_dev['score'].values\n",
        "X_test, Y_test = np.mean([feature_1_test, feature_2_test], axis=0), df_test['score'].values\n",
        "\n",
        "# Initiate a regression model and train it.\n",
        "regression_model = linear_model.Ridge(alpha=0.5)\n",
        "regression_model.fit(X_train, Y_train)\n",
        "\n",
        "# Print spearman correlation on the predicted output of the dev and test sets.\n",
        "print(\"For Train Set\", round(stats.spearmanr(regression_model.predict(X_train), Y_train)[0], 3))\n",
        "print(\"For Dev Set\", round(stats.spearmanr(regression_model.predict(X_dev), Y_dev)[0], 3))\n",
        "print(\"For Test Set\", round(stats.spearmanr(regression_model.predict(X_test), Y_test)[0], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 180/180 [51:11<00:00, 17.06s/it]\n",
            "Iteration: 100%|██████████| 180/180 [49:00<00:00, 16.34s/it]\n",
            "Epoch: 100%|██████████| 2/2 [1:45:23<00:00, 3161.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Train Set 0.928\n",
            "For Dev Set 0.897\n",
            "For Test Set 0.864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Kyode\\clg\\NLP_Assignments\\venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
            "  warnings.warn(\"The input array could not be properly \"\n"
          ]
        }
      ],
      "source": [
        "def form_data(data_frame):\n",
        "    \"\"\"\n",
        "    Input a data frame and return the dataloder.\n",
        "    \"\"\"\n",
        "    sent_a_samples = data_frame[\"sent_a\"].values\n",
        "    sent_b_samples = data_frame[\"sent_b\"].values\n",
        "    labels = data_frame[\"score\"].values.astype(float)\n",
        "    labels /= 5\n",
        "\n",
        "    labels = torch.from_numpy(labels).float()\n",
        "\n",
        "    examples = []\n",
        "    for i in range(sent_a_samples.shape[0]):\n",
        "        examples.append(InputExample(\n",
        "                            texts = [sent_a_samples[i], sent_b_samples[i]],\n",
        "                            label = labels[i]\n",
        "                        ))\n",
        "\n",
        "    dataloader = DataLoader(examples, shuffle = True, batch_size = BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "    \"\"\"\n",
        "    Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "    \"\"\"\n",
        "    x1, x2 = trained_model.encode(data_type[\"sent_a\"].values), trained_model.encode(data_type[\"sent_b\"].values)\n",
        "    return util.cos_sim(x1, x2).diagonal()\n",
        "\n",
        "\n",
        "# dataloader_train = form_data(df_train)\n",
        "# dataloader_dev = form_data(df_dev)\n",
        "# dataloader_test = form_data(df_test)\n",
        "\n",
        "# base_model = models.Transformer(HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL)\n",
        "# layer_pooling = models.Pooling(base_model.get_word_embedding_dimension())\n",
        "# layer_dense = models.Dense(in_features = layer_pooling.get_sentence_embedding_dimension(), out_features = OUT_DIM_DENSE)\n",
        "# model3 = SentenceTransformer(modules = [base_model, layer_pooling, layer_dense])\n",
        "# loss = losses.CosineSimilarityLoss(model3)\n",
        "\n",
        "# model_evaluator = EmbeddingSimilarityEvaluator(df_dev[\"sent_a\"].values, df_dev[\"sent_b\"].values, df_dev[\"score\"].values.astype(float)/5, batch_size = BATCH_SIZE)\n",
        "\n",
        "# # Fit the model3.\n",
        "# model3.fit(train_objectives = [(dataloader_train, loss)], evaluator = model_evaluator, epochs = NUM_EPOCHS, warmup_steps = NUM_WARMUP, output_path = MODEL_SAVE_PATH)\n",
        "# # Print spearman correlation on the predicted output of the dev and test sets.\n",
        "\n",
        "# We have saved the model in the MODEL_SAVE_PATH so that we can save the trouble of running the fit() function everytime\n",
        "model3 = SentenceTransformer(MODEL_SAVE_PATH)\n",
        "\n",
        "# Using the get_model_predicts() function, we can get the cosine similarity scores between the sentences\n",
        "train_preds = get_model_predicts(df_train, model3)\n",
        "dev_preds = get_model_predicts(df_dev, model3)\n",
        "test_preds = get_model_predicts(df_test, model3)\n",
        "\n",
        "# Using the stats.spearmanr function we get the spearman correlation\n",
        "print(\"For Train Set\", round(stats.spearmanr(train_preds, df_train[\"score\"].values)[0], 3))\n",
        "print(\"For Dev Set\", round(stats.spearmanr(dev_preds, df_dev[\"score\"].values)[0], 3))\n",
        "print(\"For Test Set\", round(stats.spearmanr(test_preds, df_test[\"score\"].values)[0], 3))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
